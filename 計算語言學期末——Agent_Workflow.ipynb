{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcqR32HGA7Xc"
   },
   "source": [
    "\n",
    "# æ¨™é¡Œï¼šå¥½åƒè¦å¯«äº›ä»€éº¼å¾ˆå²å®³çš„æ±è¥¿ã€‚\n",
    "---\n",
    "\n",
    "## To-do list\n",
    "### A tier: ç·Šæ€¥ä¸”é‡è¦\n",
    "- LM-AIG loop\n",
    "- æ”¶é›†æ¸¬é©—æ•¸æ“š\n",
    "- å¯« System Prompt\n",
    "### B tier: é‡è¦ä½†ä¸ç·Šæ€¥\n",
    "- Searching Agent\n",
    "- Data-analysis pipeline\n",
    "### C tier: ç·Šæ€¥ä½†ä¸é‡è¦\n",
    "- é¸æ¨¡å‹ï¼ŒåŸå§‹ç ”ç©¶ä½¿ç”¨ GPT-4oã€‚æˆ‘å€‘å¯ä»¥æ”¹ç”¨ instruct model?\n",
    "### D tier: ä¸é‡è¦ä¹Ÿä¸ç·Šæ€¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¨­å®šç’°å¢ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "qPVV2xe7A2MV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ AI è‡ªå‹•å‡ºé¡Œç³»çµ±åˆå§‹åŒ–å®Œæˆï¼\n",
      "ä½¿ç”¨æ¨¡å‹: qwen3:8b\n",
      "âœ… Ollama æœå‹™æ­£å¸¸\n",
      "å·²å®‰è£æ¨¡å‹: ['', '', '', '', '']\n",
      "âœ… æ¨¡å‹ qwen3:8b æ¸¬è©¦æˆåŠŸï¼\n",
      "æ¸¬è©¦å›æ‡‰: \n"
     ]
    }
   ],
   "source": [
    "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
    "import ollama\n",
    "import json\n",
    "import requests\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import re\n",
    "\n",
    "# è¨­å®šåŸºæœ¬é…ç½®\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"ç³»çµ±é…ç½®é¡åˆ¥\"\"\"\n",
    "    model_name: str = \"qwen3:8b\"  # ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹\n",
    "    temperature: float = 0.7\n",
    "    max_tokens: int = 1000\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"ğŸš€ AI è‡ªå‹•å‡ºé¡Œç³»çµ±åˆå§‹åŒ–å®Œæˆï¼\")\n",
    "print(f\"ä½¿ç”¨æ¨¡å‹: {config.model_name}\")\n",
    "\n",
    "# æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦é‹è¡Œ\n",
    "try:\n",
    "    models_response = ollama.list()\n",
    "    print(f\"âœ… Ollama æœå‹™æ­£å¸¸\")\n",
    "\n",
    "    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨\n",
    "    available_models = []\n",
    "    if 'models' in models_response:\n",
    "        available_models = [model.get('name', '')\n",
    "                            for model in models_response['models']]\n",
    "\n",
    "    print(f\"å·²å®‰è£æ¨¡å‹: {available_models}\")\n",
    "\n",
    "    # æ¸¬è©¦æ¨¡å‹èª¿ç”¨\n",
    "    test_response = ollama.chat(\n",
    "        model=config.model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Hello, please respond with 'System ready'\"}],\n",
    "        options={\"temperature\": 0.1, \"num_predict\": 10}\n",
    "    )\n",
    "\n",
    "    if test_response and 'message' in test_response:\n",
    "        print(f\"âœ… æ¨¡å‹ {config.model_name} æ¸¬è©¦æˆåŠŸï¼\")\n",
    "        print(f\"æ¸¬è©¦å›æ‡‰: {test_response['message']['content']}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  æ¨¡å‹å›æ‡‰æ ¼å¼ç•°å¸¸\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama é€£æ¥å¤±æ•—: {e}\")\n",
    "    print(\"è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸¦ä¸”æ¨¡å‹å·²å®‰è£\")\n",
    "    print(\"å¯ä»¥å˜—è©¦é‹è¡Œ: ollama pull llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXn2CVaqBS1e"
   },
   "source": [
    "## æœç´¢è³‡æ–™ Agent\n",
    "\n",
    "ï¼ˆä¹Ÿè¨±é€™å€‹æ­¥é©Ÿå¯ä»¥ç•¶ä½œå¯é¸çš„ï¼Œä¸å¼·è¿«é€²è¡Œæœç´¢ã€‚ï¼‰\n",
    "\n",
    "æ¥æ”¶ Prompt ä¸¦é€é Perplexity APIï¼ˆæˆ–å…¶ä»–æ±è¥¿ï¼‰æœç´¢ç¶²è·¯ã€‚\n",
    "\n",
    "å°‡æœç´¢çµæœæä¾›çµ¦ä½¿ç”¨è€…åšè©•ä¼°ï¼Œè‹¥ä½¿ç”¨è€…ä¸æ¥å—å°±é‡æ–°é€²è¡Œæœç´¢ã€‚\n",
    "\n",
    "å°‡æœç´¢çµæœä½œç‚º Prompt å‚³çµ¦ LM-AIG ä¸¦é–‹å§‹ç·¨è£½æ¸¬é©—ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlZsYWLIBWpw"
   },
   "source": [
    "## LM-AIG loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt09pt3bBf2h"
   },
   "source": [
    "### Item Writing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sahJQNH1BZYP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ItemWritingAgent å·²åˆå§‹åŒ–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "class ItemWritingAgent():\n",
    "    \"\"\"\n",
    "    åŸºæ–¼ Ollama çš„é¡Œç›®ç”Ÿæˆä»£ç†äººï¼Œæ ¹æ“šä½¿ç”¨è€…è¦æ ¼ä½¿ç”¨æŒ‡å®šèªè¨€æ¨¡å‹ç”Ÿæˆæ¸¬é©—é¡Œç›®ã€‚\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str = None, system_prompt: str = None):\n",
    "        self.model = model or config.model_name\n",
    "        self.system_prompt = system_prompt or self._default_system_prompt()\n",
    "\n",
    "    def _default_system_prompt(self) -> str:\n",
    "        \"\"\"é è¨­çš„ç³»çµ±æç¤ºè©\"\"\"\n",
    "        return \"\"\"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å¿ƒç†æ¸¬é©—é¡Œç›®ç·¨å¯«å°ˆå®¶ã€‚è«‹æ ¹æ“šä½¿ç”¨è€…çš„éœ€æ±‚ï¼Œç”Ÿæˆé«˜å“è³ªçš„æ¸¬é©—é¡Œç›®ã€‚\n",
    "\n",
    "ç”Ÿæˆè¦å‰‡ï¼š\n",
    "1. æ¯å€‹é¡Œç›®éƒ½æ‡‰è©²æœ‰æ˜ç¢ºçš„å¿ƒç†å­¸ç†è«–åŸºç¤\n",
    "2. é¡Œç›®èªè¨€æ‡‰è©²æ¸…æ™°ã€ç„¡æ­§ç¾©\n",
    "3. é¿å…æ–‡åŒ–åè¦‹å’Œäººå£å­¸åè¦‹\n",
    "4. é¡Œç›®é›£åº¦æ‡‰è©²é©ä¸­\n",
    "5. æä¾›å¤šå€‹é¸é …\n",
    "\n",
    "è¼¸å‡ºæ ¼å¼ï¼š\n",
    "è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š\n",
    "- item: é¡Œç›®å…§å®¹\n",
    "- psychological_construct: å¿ƒç†å»ºæ§‹\n",
    "\"\"\"\n",
    "\n",
    "    def generate_items(self, specifications: str, num_items: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        æ ¹æ“šè¦æ ¼ç”Ÿæˆæ¸¬é©—é¡Œç›®\n",
    "\n",
    "        Args:\n",
    "            specifications (str): ä½¿ç”¨è€…å°æ¸¬é©—é¡Œç›®çš„è¦æ ¼è¦æ±‚\n",
    "            num_items (int): è¦ç”Ÿæˆçš„é¡Œç›®æ•¸é‡\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: ç”Ÿæˆçš„é¡Œç›®è³‡æ–™ï¼ŒåŒ…å«åŸå§‹è¨Šæ¯èˆ‡è™•ç†å¾Œçš„JSONé¡Œç›®åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æ§‹å»ºå®Œæ•´çš„æç¤ºè©\n",
    "            user_prompt = f\"\"\"\n",
    "è¦æ ¼è¦æ±‚ï¼š\n",
    "{specifications}\n",
    "\n",
    "è«‹æ ¹æ“šä»¥ä¸Šè¦æ ¼ç”Ÿæˆ {num_items} å€‹å¿ƒç†æ¸¬é©—é¡Œç›®ï¼Œåš´æ ¼æŒ‰ç…§ JSON æ ¼å¼è¼¸å‡ºçµæœã€‚\n",
    "\"\"\"\n",
    "\n",
    "            # èª¿ç”¨ Ollama API\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                options={\n",
    "                    \"temperature\": config.temperature,\n",
    "                    \"num_predict\": config.max_tokens\n",
    "                },\n",
    "                # format=\"json\"\n",
    "            )\n",
    "\n",
    "            # è§£æå›æ‡‰\n",
    "            content = response['message']['content']\n",
    "\n",
    "            result = self.process_json_response(content)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"ç”Ÿæˆé¡Œç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\",\n",
    "                \"items\": []\n",
    "            }\n",
    "\n",
    "    def refine_items(self, items: List[str], feedback: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        æ ¹æ“šå›é¥‹æ”¹é€²é¡Œç›®\n",
    "\n",
    "        Args:\n",
    "            items: åŸå§‹é¡Œç›®åˆ—è¡¨\n",
    "            feedback: æ”¹é€²å»ºè­°\n",
    "\n",
    "        Returns:\n",
    "            æ”¹é€²å¾Œçš„é¡Œç›®\n",
    "        \"\"\"\n",
    "        try:\n",
    "            refine_prompt = f\"\"\"\n",
    "è«‹æ ¹æ“šä»¥ä¸‹å›é¥‹æ”¹é€²é€™äº›å¿ƒç†æ¸¬é©—é¡Œç›®ï¼š\n",
    "\n",
    "åŸå§‹è¦æ±‚ï¼š\n",
    "\n",
    "åŸå§‹é¡Œç›®ï¼š\n",
    "{json.dumps(items, ensure_ascii=False, indent=2)}\n",
    "\n",
    "æ”¹é€²å»ºè­°ï¼š\n",
    "{feedback}\n",
    "\n",
    "è«‹è¼¸å‡ºæ”¹é€²å¾Œçš„é¡Œç›®ï¼ˆJSON æ ¼å¼ï¼‰ã€‚\n",
    "\"\"\"\n",
    "\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": refine_prompt}\n",
    "                ],\n",
    "                options={\n",
    "                    \"temperature\": config.temperature,\n",
    "                    \"num_predict\": config.max_tokens\n",
    "                }\n",
    "            )\n",
    "\n",
    "            content = response['message']['content']\n",
    "\n",
    "            result = self.process_json_response(content)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"æ”¹é€²é¡Œç›®æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\",\n",
    "                \"items\": items\n",
    "            }\n",
    "\n",
    "    def process_json_response(self, content: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        è™•ç†æ¨¡å‹å›æ‡‰çš„ JSON æ ¼å¼\n",
    "\n",
    "        Args:\n",
    "            content: æ¨¡å‹å›æ‡‰å…§å®¹\n",
    "\n",
    "        Returns:\n",
    "            è§£æå¾Œçš„ JSON è³‡æ–™\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # å…ˆå»é™¤ markdown æ ¼å¼\n",
    "            content = re.sub(r\"^```json|^```|```$\", \"\",\n",
    "                             content, flags=re.MULTILINE).strip()\n",
    "            json_match = re.search(r'\\{.*\\}', content, re.DOTALL).group()\n",
    "            if json_match:\n",
    "                content = json.loads(f\"[{json_match}]\")\n",
    "                # content = json.loads(json_match)\n",
    "                result = {\"raw_output\": content, \"items\": content}\n",
    "            else:\n",
    "                result = {\"raw_output\": content, \"items\": content}\n",
    "        except json.JSONDecodeError:\n",
    "            result = {\"raw_output\": content, \"items\": content}\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "# å»ºç«‹é¡Œç›®ç”Ÿæˆå™¨å¯¦ä¾‹\n",
    "item_writer = ItemWritingAgent()\n",
    "print(\"âœ… ItemWritingAgent å·²åˆå§‹åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjHFGrcqBjhc"
   },
   "source": [
    "### Critic Agent\n",
    "\n",
    "åŒ…å«\n",
    "- Content Reviewer Agent: è©•ä¼°å…§å®¹æ•ˆåº¦ã€‚\n",
    "- Linguistic Reviewer Agent: æª¢æŸ¥é¡Œç›®çš„å¯è®€æ€§èˆ‡é€šé †åº¦ã€‚\n",
    "- Bias Reviewer Agent: æª¢æŸ¥æ˜¯å¦æœ‰äººå£å­¸ä¸Šçš„åè¦‹å•é¡Œã€‚\n",
    "- Meta Reviewer Agent: æ•´åˆä¸¦å›é¥‹çµ¦ Critic Agentã€‚\n",
    "\n",
    "Critic Agent åœ¨æ ¡æ­£å®Œä¹‹å¾Œï¼Œå†å°‡çµæœå›å‚³çµ¦ä½¿ç”¨è€…æä¾›å›é¥‹ã€‚è‹¥ä½¿ç”¨è€…èªç‚ºéœ€è¦ä¿®æ”¹ï¼Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CriticAgent å·²åˆå§‹åŒ–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "class CriticAgent:\n",
    "    \"\"\"\n",
    "    è©•å¯©ä»£ç†äººï¼ŒåŒ…å«å¤šå€‹å­è©•å¯©å“¡ä¾†è©•ä¼°æ¸¬é©—é¡Œç›®çš„å“è³ª\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: str = None):\n",
    "        self.model = model or config.model_name\n",
    "        self.content_reviewer = ContentReviewer(self.model)\n",
    "        self.linguistic_reviewer = LinguisticReviewer(self.model)\n",
    "        self.bias_reviewer = BiasReviewer(self.model)\n",
    "        self.meta_reviewer = MetaReviewer(self.model)\n",
    "\n",
    "    def review_items(self, items: List[str], construct: str = \"\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å…¨é¢è©•å¯©æ¸¬é©—é¡Œç›®\n",
    "\n",
    "        Args:\n",
    "            items: é¡Œç›®åˆ—è¡¨\n",
    "            construct: å¿ƒç†å»ºæ§‹åç¨±\n",
    "\n",
    "        Returns:\n",
    "            è©•å¯©çµæœ\n",
    "        \"\"\"\n",
    "\n",
    "        # å„å­è©•å¯©å“¡çš„è©•å¯©çµæœ\n",
    "        reviews = {\n",
    "            \"content_review\": self.content_reviewer.review(items, construct),\n",
    "            \"linguistic_review\": self.linguistic_reviewer.review(items),\n",
    "            \"bias_review\": self.bias_reviewer.review(items),\n",
    "        }\n",
    "\n",
    "        # å…ƒè©•å¯©å“¡æ•´åˆæ‰€æœ‰è©•å¯©çµæœ\n",
    "        meta_review = self.meta_reviewer.integrate_reviews(reviews, items)\n",
    "\n",
    "        return {\n",
    "            \"individual_reviews\": reviews,\n",
    "            \"meta_review\": meta_review,\n",
    "            \"overall_score\": meta_review.get(\"overall_score\", 0),\n",
    "            \"recommendations\": meta_review.get(\"recommendations\", [])\n",
    "        }\n",
    "\n",
    "\n",
    "class ContentReviewer:\n",
    "    \"\"\"å…§å®¹æ•ˆåº¦è©•å¯©å“¡\"\"\"\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "\n",
    "    def review(self, items: List[str], construct: str) -> Dict[str, Any]:\n",
    "        \"\"\"è©•ä¼°å…§å®¹æ•ˆåº¦\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "ä½ æ˜¯å¿ƒç†æ¸¬é©—å…§å®¹æ•ˆåº¦å°ˆå®¶ã€‚è«‹è©•ä¼°ä»¥ä¸‹é¡Œç›®æ˜¯å¦èƒ½æœ‰æ•ˆæ¸¬é‡æŒ‡å®šçš„å¿ƒç†å»ºæ§‹ã€‚\n",
    "\n",
    "å¿ƒç†å»ºæ§‹: {construct}\n",
    "é¡Œç›®åˆ—è¡¨:\n",
    "{json.dumps(items, ensure_ascii=False, indent=2)}\n",
    "\n",
    "è«‹è©•ä¼°ï¼š\n",
    "1. é¡Œç›®æ˜¯å¦èˆ‡å¿ƒç†å»ºæ§‹ç›¸é—œ\n",
    "2. é¡Œç›®æ˜¯å¦æ¶µè“‹è©²å»ºæ§‹çš„é‡è¦é¢å‘\n",
    "3. é¡Œç›®çš„ç†è«–åŸºç¤æ˜¯å¦å……åˆ†\n",
    "\n",
    "è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºè©•ä¼°çµæœï¼ŒåŒ…å«ï¼š\n",
    "- validity_score: æ•ˆåº¦åˆ†æ•¸ (1-10)\n",
    "- strengths: å„ªé»åˆ—è¡¨\n",
    "- weaknesses: ç¼ºé»åˆ—è¡¨\n",
    "- suggestions: æ”¹é€²å»ºè­°\n",
    "\"\"\"\n",
    "\n",
    "        return self._get_review_response(prompt)\n",
    "\n",
    "    def _get_review_response(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–è©•å¯©å›æ‡‰\"\"\"\n",
    "        print(\"ğŸ“ å…§å®¹æ•ˆåº¦è©•å¯©ä¸­...\")\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.3},\n",
    "                # format=\"json\"\n",
    "            )\n",
    "\n",
    "            content = response['message']['content']\n",
    "\n",
    "            # å˜—è©¦è§£æ JSON\n",
    "            try:\n",
    "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "                else:\n",
    "                    result = {\"raw_output\": content, \"validity_score\": 5}\n",
    "            except json.JSONDecodeError:\n",
    "                result = {\"raw_output\": content, \"validity_score\": 5}\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"validity_score\": 0}\n",
    "\n",
    "\n",
    "class LinguisticReviewer:\n",
    "    \"\"\"èªè¨€å­¸è©•å¯©å“¡\"\"\"\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "\n",
    "    def review(self, items: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"è©•ä¼°é¡Œç›®çš„èªè¨€å“è³ª\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "ä½ æ˜¯èªè¨€å­¸å°ˆå®¶ã€‚è«‹è©•ä¼°ä»¥ä¸‹æ¸¬é©—é¡Œç›®çš„èªè¨€å“è³ªã€‚\n",
    "\n",
    "é¡Œç›®åˆ—è¡¨:\n",
    "{json.dumps(items, ensure_ascii=False, indent=2)}\n",
    "\n",
    "è«‹è©•ä¼°ï¼š\n",
    "1. èªè¨€æ˜¯å¦æ¸…æ™°æ˜“æ‡‚\n",
    "2. æ˜¯å¦æœ‰èªæ³•éŒ¯èª¤\n",
    "3. ç”¨è©æ˜¯å¦æ°ç•¶\n",
    "4. æ˜¯å¦æœ‰æ­§ç¾©è¡¨é”\n",
    "\n",
    "è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºè©•ä¼°çµæœï¼ŒåŒ…å«ï¼š\n",
    "- readability_score: å¯è®€æ€§åˆ†æ•¸ (1-10)\n",
    "- grammar_issues: èªæ³•å•é¡Œåˆ—è¡¨\n",
    "- clarity_issues: æ¸…æ™°åº¦å•é¡Œåˆ—è¡¨\n",
    "- suggestions: èªè¨€æ”¹é€²å»ºè­°\n",
    "\"\"\"\n",
    "\n",
    "        return self._get_review_response(prompt)\n",
    "\n",
    "    def _get_review_response(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–è©•å¯©å›æ‡‰\"\"\"\n",
    "        print(\"ğŸ“ èªè¨€å­¸è©•å¯©ä¸­...\")\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.3},\n",
    "                # format=\"json\"\n",
    "            )\n",
    "\n",
    "            content = response['message']['content']\n",
    "\n",
    "            try:\n",
    "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "                else:\n",
    "                    result = {\"raw_output\": content, \"readability_score\": 5}\n",
    "            except json.JSONDecodeError:\n",
    "                result = {\"raw_output\": content, \"readability_score\": 5}\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"readability_score\": 0}\n",
    "\n",
    "\n",
    "class BiasReviewer:\n",
    "    \"\"\"åè¦‹æª¢æŸ¥è©•å¯©å“¡\"\"\"\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "\n",
    "    def review(self, items: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"æª¢æŸ¥äººå£å­¸åè¦‹\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "ä½ æ˜¯æ¸¬é©—åè¦‹æª¢æŸ¥å°ˆå®¶ã€‚è«‹æª¢æŸ¥ä»¥ä¸‹æ¸¬é©—é¡Œç›®æ˜¯å¦å­˜åœ¨äººå£å­¸åè¦‹ã€‚\n",
    "\n",
    "é¡Œç›®åˆ—è¡¨:\n",
    "{json.dumps(items, ensure_ascii=False, indent=2)}\n",
    "\n",
    "è«‹æª¢æŸ¥æ˜¯å¦å­˜åœ¨ä»¥ä¸‹åè¦‹ï¼š\n",
    "1. æ€§åˆ¥åè¦‹\n",
    "2. å¹´é½¡åè¦‹  \n",
    "3. æ–‡åŒ–åè¦‹\n",
    "4. ç¤¾ç¶“åœ°ä½åè¦‹\n",
    "5. å…¶ä»–æ­§è¦–æ€§å…§å®¹\n",
    "\n",
    "è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºæª¢æŸ¥çµæœï¼ŒåŒ…å«ï¼š\n",
    "- bias_score: åè¦‹ç¨‹åº¦ (1-10, 10è¡¨ç¤ºç„¡åè¦‹)\n",
    "- detected_biases: ç™¼ç¾çš„åè¦‹é¡å‹åˆ—è¡¨\n",
    "- problematic_items: æœ‰å•é¡Œçš„é¡Œç›®\n",
    "- suggestions: æ¶ˆé™¤åè¦‹çš„å»ºè­°\n",
    "\"\"\"\n",
    "\n",
    "        return self._get_review_response(prompt)\n",
    "\n",
    "    def _get_review_response(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–è©•å¯©å›æ‡‰\"\"\"\n",
    "        print(\"ğŸ“ åè¦‹æª¢æŸ¥è©•å¯©ä¸­...\")\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.3},\n",
    "                # format=\"json\"\n",
    "            )\n",
    "\n",
    "            content = response['message']['content']\n",
    "\n",
    "            try:\n",
    "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "                else:\n",
    "                    result = {\"raw_output\": content, \"bias_score\": 5}\n",
    "            except json.JSONDecodeError:\n",
    "                result = {\"raw_output\": content, \"bias_score\": 5}\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"bias_score\": 0}\n",
    "\n",
    "\n",
    "class MetaReviewer:\n",
    "    \"\"\"å…ƒè©•å¯©å“¡ï¼Œæ•´åˆæ‰€æœ‰è©•å¯©çµæœ\"\"\"\n",
    "\n",
    "    def __init__(self, model: str):\n",
    "        self.model = model\n",
    "\n",
    "    def integrate_reviews(self, reviews: Dict[str, Any], items: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"æ•´åˆæ‰€æœ‰è©•å¯©çµæœ\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "ä½ æ˜¯æ¸¬é©—è©•å¯©çš„è³‡æ·±å°ˆå®¶ã€‚è«‹æ•´åˆä»¥ä¸‹å„å€‹è©•å¯©å“¡çš„è©•å¯©çµæœï¼Œçµ¦å‡ºç¶œåˆè©•åƒ¹å’Œæ”¹é€²å»ºè­°ã€‚\n",
    "\n",
    "åŸå§‹é¡Œç›®:\n",
    "{json.dumps(items, ensure_ascii=False, indent=2)}\n",
    "\n",
    "å„è©•å¯©å“¡çµæœ:\n",
    "å…§å®¹æ•ˆåº¦è©•å¯©: {json.dumps(reviews.get('content_review', {}), ensure_ascii=False, indent=2)}\n",
    "èªè¨€å­¸è©•å¯©: {json.dumps(reviews.get('linguistic_review', {}), ensure_ascii=False, indent=2)}\n",
    "åè¦‹æª¢æŸ¥è©•å¯©: {json.dumps(reviews.get('bias_review', {}), ensure_ascii=False, indent=2)}\n",
    "\n",
    "è«‹æä¾›ï¼š\n",
    "1. ç¶œåˆè©•åˆ† (1-10)\n",
    "2. ä¸»è¦å„ªé»\n",
    "3. ä¸»è¦å•é¡Œ\n",
    "4. å„ªå…ˆæ”¹é€²å»ºè­°\n",
    "5. æ˜¯å¦å»ºè­°é‡æ–°ç”Ÿæˆ\n",
    "\n",
    "è«‹ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼ŒåŒ…å«ï¼š\n",
    "- overall_score: ç¶œåˆåˆ†æ•¸\n",
    "- strengths: ä¸»è¦å„ªé»åˆ—è¡¨\n",
    "- major_issues: ä¸»è¦å•é¡Œåˆ—è¡¨\n",
    "- recommendations: æ”¹é€²å»ºè­°åˆ—è¡¨\n",
    "- regenerate_recommended: æ˜¯å¦å»ºè­°é‡æ–°ç”Ÿæˆ (boolean)\n",
    "\"\"\"\n",
    "\n",
    "        return self._get_review_response(prompt)\n",
    "\n",
    "    def _get_review_response(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"ç²å–è©•å¯©å›æ‡‰\"\"\"\n",
    "        try:\n",
    "            response = ollama.chat(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.3},\n",
    "                # format=\"json\"\n",
    "            )\n",
    "\n",
    "            content = response['message']['content']\n",
    "\n",
    "            try:\n",
    "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "                else:\n",
    "                    result = {\"raw_output\": content, \"overall_score\": 5}\n",
    "            except json.JSONDecodeError:\n",
    "                result = {\"raw_output\": content, \"overall_score\": 5}\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e), \"overall_score\": 0}\n",
    "\n",
    "\n",
    "# å»ºç«‹è©•å¯©ä»£ç†äººå¯¦ä¾‹\n",
    "critic_agent = CriticAgent()\n",
    "print(\"âœ… CriticAgent å·²åˆå§‹åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS4h-9H9CDGw"
   },
   "source": [
    "## è³‡æ–™åˆ†æ\n",
    "\n",
    "æ ¹æ“šå‰è¿°çš„ç†è«–æ¶æ§‹è‡ªå‹•é€²è¡Œ Factor analysisã€‚\n",
    "\n",
    "ï¼ˆMCP æ‡‰è©²ç”¨åœ¨é€™è£¡ï¼Œè®“æ¨¡å‹å‚³å…¥EFA, CFA çš„å‡½å¼åƒæ•¸ï¼Œä¸¦èª¿ç”¨å‡½æ•¸å–å¾—çµæœã€‚ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "m7kLMPE7Fhma"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataAnalysisPipeline å·²åˆå§‹åŒ–å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "\n",
    "class DataAnalysisPipeline:\n",
    "    \"\"\"\n",
    "    è³‡æ–™åˆ†æç®¡ç·šï¼Œç”¨æ–¼åˆ†ææ¸¬é©—é¡Œç›®çš„å¿ƒç†è¨ˆé‡ç‰¹æ€§\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.factor_analysis = None\n",
    "        self.data = None\n",
    "        self.scaled_data = None\n",
    "\n",
    "    def load_simulated_data(self, n_participants: int = 200, n_items: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        ç”Ÿæˆæ¨¡æ“¬æ¸¬é©—è³‡æ–™ï¼ˆå¯¦éš›ä½¿ç”¨æ™‚æ‡‰è¼‰å…¥çœŸå¯¦è³‡æ–™ï¼‰\n",
    "\n",
    "        Args:\n",
    "            n_participants: å—è©¦è€…æ•¸é‡\n",
    "            n_items: é¡Œç›®æ•¸é‡\n",
    "\n",
    "        Returns:\n",
    "            æ¨¡æ“¬çš„æ¸¬é©—åæ‡‰è³‡æ–™\n",
    "        \"\"\"\n",
    "        # ç”Ÿæˆæ¨¡æ“¬è³‡æ–™ï¼šå‡è¨­æœ‰ä¸€å€‹æ½›åœ¨å› å­\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # æ½›åœ¨å› å­åˆ†æ•¸\n",
    "        latent_factor = np.random.normal(0, 1, n_participants)\n",
    "\n",
    "        # é¡Œç›®åƒæ•¸\n",
    "        item_loadings = np.random.uniform(0.3, 0.8, n_items)\n",
    "        item_difficulties = np.random.uniform(-2, 2, n_items)\n",
    "\n",
    "        # ç”Ÿæˆåæ‡‰è³‡æ–™ï¼ˆä½¿ç”¨ Rasch æ¨¡å‹æ¦‚å¿µä½†ç°¡åŒ–ç‚ºé€£çºŒåˆ†æ•¸ï¼‰\n",
    "        responses = []\n",
    "        for i in range(n_items):\n",
    "            # ç°¡åŒ–çš„é …ç›®åæ‡‰ï¼šå› å­è² è·é‡ * æ½›åœ¨å› å­ + é›£åº¦ + éš¨æ©Ÿèª¤å·®\n",
    "            item_scores = (item_loadings[i] * latent_factor +\n",
    "                           item_difficulties[i] +\n",
    "                           np.random.normal(0, 0.3, n_participants))\n",
    "            # è½‰æ›ç‚º 1-5 åˆ†çš„æå…‹ç‰¹é‡è¡¨\n",
    "            item_scores = np.clip(np.round(3 + item_scores), 1, 5)\n",
    "            responses.append(item_scores)\n",
    "\n",
    "        # å»ºç«‹ DataFrame\n",
    "        data = pd.DataFrame(\n",
    "            np.array(responses).T,\n",
    "            columns=[f'Item_{i+1}' for i in range(n_items)]\n",
    "        )\n",
    "\n",
    "        self.data = data\n",
    "        return data\n",
    "\n",
    "    def exploratory_factor_analysis(self, n_factors: int = 2) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        é€²è¡Œæ¢ç´¢æ€§å› å­åˆ†æ (EFA)\n",
    "\n",
    "        Args:\n",
    "            n_factors: å› å­æ•¸é‡\n",
    "\n",
    "        Returns:\n",
    "            å› å­åˆ†æçµæœ\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            return {\"error\": \"è«‹å…ˆè¼‰å…¥è³‡æ–™\"}\n",
    "\n",
    "        try:\n",
    "            # æ¨™æº–åŒ–è³‡æ–™\n",
    "            self.scaled_data = self.scaler.fit_transform(self.data)\n",
    "\n",
    "            # å› å­åˆ†æ\n",
    "            self.factor_analysis = FactorAnalysis(\n",
    "                n_components=n_factors, random_state=42)\n",
    "            factor_scores = self.factor_analysis.fit_transform(\n",
    "                self.scaled_data)\n",
    "\n",
    "            # å› å­è² è·é‡\n",
    "            loadings = self.factor_analysis.components_.T\n",
    "\n",
    "            # è¨ˆç®—è§£é‡‹è®Šç•°é‡\n",
    "            explained_variance = np.var(factor_scores, axis=0)\n",
    "            explained_variance_ratio = explained_variance / \\\n",
    "                np.sum(explained_variance)\n",
    "\n",
    "            # è¨ˆç®—é …ç›®é–“ç›¸é—œ\n",
    "            correlation_matrix = np.corrcoef(self.data.T)\n",
    "\n",
    "            # ä¿¡åº¦åˆ†æï¼ˆCronbach's Alphaï¼‰\n",
    "            alpha = self._calculate_cronbach_alpha()\n",
    "\n",
    "            results = {\n",
    "                \"n_factors\": n_factors,\n",
    "                \"factor_loadings\": loadings.tolist(),\n",
    "                \"factor_loadings_df\": pd.DataFrame(\n",
    "                    loadings,\n",
    "                    columns=[f'Factor_{i+1}' for i in range(n_factors)],\n",
    "                    index=self.data.columns\n",
    "                ),\n",
    "                \"explained_variance_ratio\": explained_variance_ratio.tolist(),\n",
    "                \"correlation_matrix\": correlation_matrix.tolist(),\n",
    "                \"cronbach_alpha\": alpha,\n",
    "                \"factor_scores\": factor_scores,\n",
    "                \"item_statistics\": self._calculate_item_statistics()\n",
    "            }\n",
    "\n",
    "            return results\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"å› å­åˆ†æå¤±æ•—: {str(e)}\"}\n",
    "\n",
    "    def _calculate_cronbach_alpha(self) -> float:\n",
    "        \"\"\"è¨ˆç®— Cronbach's Alpha ä¿¡åº¦ä¿‚æ•¸\"\"\"\n",
    "        if self.data is None:\n",
    "            return 0.0\n",
    "\n",
    "        # é …ç›®æ•¸é‡\n",
    "        k = self.data.shape[1]\n",
    "\n",
    "        # é …ç›®è®Šç•°æ•¸\n",
    "        item_variances = self.data.var(axis=0, ddof=1)\n",
    "\n",
    "        # ç¸½åˆ†è®Šç•°æ•¸\n",
    "        total_variance = self.data.sum(axis=1).var(ddof=1)\n",
    "\n",
    "        # Cronbach's Alpha å…¬å¼\n",
    "        alpha = (k / (k - 1)) * (1 - item_variances.sum() / total_variance)\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def _calculate_item_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"è¨ˆç®—é¡Œç›®çµ±è¨ˆé‡\"\"\"\n",
    "        if self.data is None:\n",
    "            return {}\n",
    "\n",
    "        stats = {\n",
    "            \"means\": self.data.mean().to_dict(),\n",
    "            \"std_devs\": self.data.std().to_dict(),\n",
    "            \"skewness\": self.data.skew().to_dict(),\n",
    "            \"kurtosis\": self.data.kurtosis().to_dict(),\n",
    "        }\n",
    "\n",
    "        # é …ç›®-ç¸½åˆ†ç›¸é—œ\n",
    "        total_scores = self.data.sum(axis=1)\n",
    "        item_total_correlations = {}\n",
    "        for col in self.data.columns:\n",
    "            # ä¿®æ­£çš„é …ç›®-ç¸½åˆ†ç›¸é—œï¼ˆæ’é™¤è©²é¡Œç›®æœ¬èº«ï¼‰\n",
    "            corrected_total = total_scores - self.data[col]\n",
    "            correlation = np.corrcoef(self.data[col], corrected_total)[0, 1]\n",
    "            item_total_correlations[col] = correlation\n",
    "\n",
    "        stats[\"item_total_correlations\"] = item_total_correlations\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def generate_analysis_report(self, efa_results: Dict[str, Any]) -> str:\n",
    "        \"\"\"ç”Ÿæˆåˆ†æå ±å‘Š\"\"\"\n",
    "        if \"error\" in efa_results:\n",
    "            return f\"åˆ†æéŒ¯èª¤: {efa_results['error']}\"\n",
    "\n",
    "        report = f\"\"\"\n",
    "ğŸ“Š æ¸¬é©—å¿ƒç†è¨ˆé‡åˆ†æå ±å‘Š\n",
    "{'='*50}\n",
    "\n",
    "ğŸ”¢ åŸºæœ¬è³‡è¨Š:\n",
    "- æ¨£æœ¬æ•¸: {self.data.shape[0]}\n",
    "- é¡Œç›®æ•¸: {self.data.shape[1]}\n",
    "- å› å­æ•¸: {efa_results['n_factors']}\n",
    "\n",
    "ğŸ“ˆ ä¿¡åº¦åˆ†æ:\n",
    "- Cronbach's Alpha: {efa_results['cronbach_alpha']:.3f}\n",
    "\n",
    "ğŸ¯ å› å­åˆ†æçµæœ:\n",
    "è§£é‡‹è®Šç•°é‡æ¯”ä¾‹: {[f'{ratio:.3f}' for ratio in efa_results['explained_variance_ratio']]}\n",
    "\n",
    "ğŸ“‹ å› å­è² è·é‡:\n",
    "{efa_results['factor_loadings_df'].round(3).to_string()}\n",
    "\n",
    "ğŸ“Š é¡Œç›®çµ±è¨ˆ:\n",
    "\"\"\"\n",
    "\n",
    "        item_stats = efa_results['item_statistics']\n",
    "        for item in self.data.columns:\n",
    "            report += f\"\\n{item}:\"\n",
    "            report += f\"  å¹³å‡æ•¸: {item_stats['means'][item]:.2f}\"\n",
    "            report += f\"  æ¨™æº–å·®: {item_stats['std_devs'][item]:.2f}\"\n",
    "            report += f\"  é …ç›®-ç¸½åˆ†ç›¸é—œ: {item_stats['item_total_correlations'][item]:.3f}\"\n",
    "\n",
    "        # è©•ä¼°å»ºè­°\n",
    "        report += f\"\\n\\nğŸ’¡ è©•ä¼°å»ºè­°:\\n\"\n",
    "\n",
    "        if efa_results['cronbach_alpha'] >= 0.8:\n",
    "            report += \"âœ… ä¿¡åº¦è‰¯å¥½ (Î± â‰¥ 0.8)\\n\"\n",
    "        elif efa_results['cronbach_alpha'] >= 0.7:\n",
    "            report += \"âš ï¸ ä¿¡åº¦å°šå¯ (0.7 â‰¤ Î± < 0.8)ï¼Œå¯è€ƒæ…®æ”¹é€²\\n\"\n",
    "        else:\n",
    "            report += \"âŒ ä¿¡åº¦åä½ (Î± < 0.7)ï¼Œå»ºè­°é‡æ–°æª¢è¦–é¡Œç›®\\n\"\n",
    "\n",
    "        # æª¢æŸ¥é …ç›®-ç¸½åˆ†ç›¸é—œ\n",
    "        low_correlation_items = [\n",
    "            item for item, corr in item_stats['item_total_correlations'].items()\n",
    "            if corr < 0.3\n",
    "        ]\n",
    "\n",
    "        if low_correlation_items:\n",
    "            report += f\"âš ï¸ ä»¥ä¸‹é¡Œç›®èˆ‡ç¸½åˆ†ç›¸é—œåä½ï¼Œå»ºè­°æª¢è¦–: {', '.join(low_correlation_items)}\\n\"\n",
    "        else:\n",
    "            report += \"âœ… æ‰€æœ‰é¡Œç›®èˆ‡ç¸½åˆ†ç›¸é—œè‰¯å¥½\\n\"\n",
    "\n",
    "        return report\n",
    "\n",
    "\n",
    "# å»ºç«‹è³‡æ–™åˆ†æç®¡ç·šå¯¦ä¾‹\n",
    "analysis_pipeline = DataAnalysisPipeline()\n",
    "print(\"âœ… DataAnalysisPipeline å·²åˆå§‹åŒ–å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹\n",
    "\n",
    "ç¾åœ¨è®“æˆ‘å€‘å°‡æ‰€æœ‰çµ„ä»¶æ•´åˆï¼Œå±•ç¤ºå®Œæ•´çš„ LM-AIG å·¥ä½œæµç¨‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LM-AIG å®Œæ•´ç³»çµ±å·²åˆå§‹åŒ–ï¼\n"
     ]
    }
   ],
   "source": [
    "class LM_AIG_System:\n",
    "    \"\"\"\n",
    "    å®Œæ•´çš„ LM-AIG ç³»çµ±ï¼Œæ•´åˆé¡Œç›®ç”Ÿæˆã€è©•å¯©å’Œè³‡æ–™åˆ†æ\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.item_writer = ItemWritingAgent()\n",
    "        self.critic = CriticAgent()\n",
    "        self.analyzer = DataAnalysisPipeline()\n",
    "\n",
    "    def run_complete_workflow(self, specifications: str, num_items: int = 5,\n",
    "                              max_iterations: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        åŸ·è¡Œå®Œæ•´çš„å·¥ä½œæµç¨‹\n",
    "\n",
    "        Args:\n",
    "            specifications: é¡Œç›®è¦æ ¼\n",
    "            num_items: é¡Œç›®æ•¸é‡\n",
    "            max_iterations: æœ€å¤§æ”¹é€²è¿­ä»£æ¬¡æ•¸\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            {\n",
    "                \"original_specifications\": str,\n",
    "                \"iterations\": List[Dict],\n",
    "                \"final_items\": List[str],\n",
    "                \"analysis_results\": Dict[str, Any]\n",
    "            }\n",
    "        \"\"\"\n",
    "        workflow_results = {\n",
    "            \"original_specifications\": specifications,\n",
    "            \"iterations\": [],\n",
    "            \"final_items\": None,\n",
    "            \"analysis_results\": None\n",
    "        }\n",
    "\n",
    "        print(f\"ğŸš€ é–‹å§‹ LM-AIG å·¥ä½œæµç¨‹\")\n",
    "        print(f\"ğŸ“ è¦æ ¼: {specifications}\")\n",
    "        print(f\"ğŸ”¢ ç›®æ¨™é¡Œç›®æ•¸é‡: {num_items}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        current_items = None\n",
    "        previous_review = {}\n",
    "        review_result = {}\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            print(f\"\\nğŸ”„ ç¬¬ {iteration + 1} æ¬¡è¿­ä»£\")\n",
    "\n",
    "            # ç¬¬ä¸€æ¬¡è¿­ä»£ï¼šç”Ÿæˆé¡Œç›®ï¼›å¾ŒçºŒè¿­ä»£ï¼šæ”¹é€²é¡Œç›®\n",
    "            if iteration == 0:\n",
    "                print(\"ğŸ“ ç”Ÿæˆåˆå§‹é¡Œç›®...\")\n",
    "                generation_result = self.item_writer.generate_items(\n",
    "                    specifications, num_items)\n",
    "            else:\n",
    "                print(\"ğŸ”§ æ ¹æ“šè©•å¯©å»ºè­°æ”¹é€²é¡Œç›®...\")\n",
    "                feedback = previous_review.get(\"recommendations\", \"è«‹æ”¹é€²é¡Œç›®å“è³ª\")\n",
    "                generation_result = self.item_writer.refine_items(\n",
    "                    current_items, str(feedback))\n",
    "\n",
    "            if \"error\" in generation_result:\n",
    "                print(f\"âŒ ç”ŸæˆéŒ¯èª¤: {generation_result['error']}\")\n",
    "                continue\n",
    "\n",
    "            current_items = generation_result.get(\"items\", [])\n",
    "            print(f\"âœ… å·²ç”Ÿæˆ {len(current_items)} å€‹é¡Œç›®\")\n",
    "            print(f\"é¡Œç›®å…§å®¹:\")\n",
    "            for i, item in enumerate(current_items, 1):\n",
    "                print(f\"{i:2}. {item.get('item') if isinstance(item, dict) else item}\")\n",
    "\n",
    "            # è©•å¯©é¡Œç›®\n",
    "            print(\"ğŸ” è©•å¯©é¡Œç›®å“è³ª...\")\n",
    "            review_result = self.critic.review_items(current_items,\n",
    "                                                     specifications)\n",
    "\n",
    "            iteration_result = {\n",
    "                \"iteration\": iteration + 1,\n",
    "                \"generated_items\": current_items,\n",
    "                \"generation_result\": generation_result,\n",
    "                \"review_result\": review_result,\n",
    "                \"overall_score\": review_result.get(\"overall_score\", 0)\n",
    "            }\n",
    "\n",
    "            # é¡¯ç¤º reveiw result\n",
    "            print(f\"\"\"ğŸ” è©•å¯©çµæœ: \n",
    "                  å…§å®¹æ•ˆåº¦è©•ä¼°ï¼š{review_result.get(\"individual_reviews\").get(\"content_review\")}\n",
    "                    èªè¨€å­¸è©•ä¼°ï¼š{review_result.get(\"individual_reviews\").get(\"linguistic_review\")}\n",
    "                    åè¦‹æª¢æŸ¥è©•ä¼°ï¼š{review_result.get(\"individual_reviews\").get(\"bias_review\")}\n",
    "                    å…ƒè©•å¯©çµæœï¼š{review_result.get(\"meta_review\")}\"\"\")\n",
    "\n",
    "            workflow_results[\"iterations\"].append(iteration_result)\n",
    "\n",
    "            print(f\"ğŸ“Š ç¶œåˆè©•åˆ†: {review_result.get('overall_score', 0)}/10\")\n",
    "\n",
    "            # å…ˆæª¢æŸ¥é¡Œç›®æ•¸ï¼Œå†æª¢æŸ¥æ˜¯å¦é”åˆ°æ»¿æ„æ¨™æº–\n",
    "            if len(current_items) < num_items:\n",
    "                print(f\"âŒ é¡Œç›®æ•¸é‡ä¸è¶³ (éœ€è¦ {num_items}ï¼Œä½†åªæœ‰ {len(current_items)})ï¼Œç¹¼çºŒæ”¹é€²\")\n",
    "                previous_review = review_result.get(\"meta_review\", {})\n",
    "                continue\n",
    "            elif review_result.get(\"overall_score\", 0) >= 7:  # 7åˆ†ä»¥ä¸Šç®—åŠæ ¼\n",
    "                print(\"âœ… é¡Œç›®å“è³ªå·²é”æ¨™æº–ï¼ŒçµæŸè¿­ä»£\")\n",
    "                break\n",
    "            elif not review_result.get(\"meta_review\", {}).get(\"regenerate_recommended\", True):\n",
    "                print(\"âœ… è©•å¯©å»ºè­°ç¹¼çºŒä½¿ç”¨ç•¶å‰ç‰ˆæœ¬\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"âš ï¸ éœ€è¦ç¹¼çºŒæ”¹é€²\")\n",
    "                previous_review = review_result.get(\"meta_review\", {})\n",
    "\n",
    "        workflow_results[\"final_items\"] = current_items\n",
    "\n",
    "        # é€²è¡Œè³‡æ–™åˆ†æï¼ˆä½¿ç”¨æ¨¡æ“¬è³‡æ–™ï¼‰\n",
    "        if current_items:\n",
    "            print(\"\\nğŸ“Š é€²è¡Œå¿ƒç†è¨ˆé‡åˆ†æ...\")\n",
    "            simulated_data = self.analyzer.load_simulated_data(\n",
    "                n_participants=200,\n",
    "                n_items=len(current_items)\n",
    "            )\n",
    "\n",
    "            efa_results = self.analyzer.exploratory_factor_analysis(\n",
    "                n_factors=1)\n",
    "            analysis_report = self.analyzer.generate_analysis_report(\n",
    "                efa_results)\n",
    "\n",
    "            workflow_results[\"analysis_results\"] = {\n",
    "                \"efa_results\": efa_results,\n",
    "                \"analysis_report\": analysis_report,\n",
    "                \"simulated_data_shape\": simulated_data.shape\n",
    "            }\n",
    "\n",
    "            print(\"âœ… åˆ†æå®Œæˆ\")\n",
    "\n",
    "        return workflow_results\n",
    "\n",
    "    def display_results(self, results: Dict[str, Any]):\n",
    "        \"\"\"é¡¯ç¤ºå·¥ä½œæµç¨‹çµæœ\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ LM-AIG ç³»çµ±åŸ·è¡Œçµæœ\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(f\"\\nğŸ“ åŸå§‹è¦æ ¼: {results['original_specifications']}\")\n",
    "        print(f\"ğŸ”„ è¿­ä»£æ¬¡æ•¸: {len(results['iterations'])}\")\n",
    "\n",
    "        # é¡¯ç¤ºæœ€çµ‚é¡Œç›®\n",
    "        final_items = results.get(\"final_items\", [])\n",
    "\n",
    "        # å¦‚æœæ˜¯å­—ä¸² JSON æ ¼å¼ï¼Œå…ˆè§£æç‚ºåˆ—è¡¨\n",
    "        if isinstance(final_items, str):\n",
    "            try:\n",
    "                final_items = json.loads(final_items)\n",
    "            except json.JSONDecodeError:\n",
    "                final_items = [final_items]\n",
    "\n",
    "        if final_items:\n",
    "            print(f\"\\nğŸ“Š æœ€çµ‚é¡Œç›® ({len(final_items)} å€‹):\")\n",
    "        for i, item in enumerate(final_items, 1):\n",
    "            print(f\"{i:2}. {item}\")\n",
    "\n",
    "        # é¡¯ç¤ºè©•å¯©æ­·ç¨‹\n",
    "        print(f\"\\nğŸ“ˆ è©•åˆ†æ­·ç¨‹:\")\n",
    "        for iteration in results[\"iterations\"]:\n",
    "            score = iteration.get(\"overall_score\", 0)\n",
    "            print(f\"  ç¬¬ {iteration['iteration']} æ¬¡è¿­ä»£: {score}/10\")\n",
    "\n",
    "        # é¡¯ç¤ºåˆ†æçµæœ\n",
    "        if results.get(\"analysis_results\"):\n",
    "            print(\"\\n\" + \"=\"*40)\n",
    "            print(\"ğŸ“Š å¿ƒç†è¨ˆé‡åˆ†æçµæœ\")\n",
    "            print(\"=\"*40)\n",
    "            print(results[\"analysis_results\"][\"analysis_report\"])\n",
    "\n",
    "\n",
    "# å»ºç«‹å®Œæ•´ç³»çµ±\n",
    "lm_aig_system = LM_AIG_System()\n",
    "print(\"âœ… LM-AIG å®Œæ•´ç³»çµ±å·²åˆå§‹åŒ–ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç³»çµ±æ¼”ç¤º\n",
    "\n",
    "è®“æˆ‘å€‘é‹è¡Œä¸€å€‹å¯¦éš›çš„ä¾‹å­ä¾†æ¸¬è©¦ç³»çµ±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ é–‹å§‹æ¼”ç¤ºï¼šBrainrot å½±éŸ¿ç¨‹åº¦é‡è¡¨ç”Ÿæˆ\n",
      "============================================================\n",
      "ğŸš€ é–‹å§‹ LM-AIG å·¥ä½œæµç¨‹\n",
      "ğŸ“ è¦æ ¼: \n",
      "è«‹è¨­è¨ˆä¸€å€‹ã€ŒBrainrot å½±éŸ¿ç¨‹åº¦é‡è¡¨ã€ï¼ˆBrainrot Impact Scaleï¼‰ï¼Œç”¨æ–¼è©•ä¼°å€‹äººå› é•·æœŸæ¥è§¸ä½å“è³ªæ•¸ä½å…§å®¹ï¼ˆå¦‚çŸ­å½±ç‰‡ã€ç¢ç‰‡è³‡è¨Šï¼‰è€Œç”¢ç”Ÿçš„è…¦è…å½±éŸ¿ã€‚\n",
      "\n",
      "æ ¸å¿ƒæ§‹å¿µï¼š\n",
      "- èªçŸ¥è¡°é€€ï¼šå°ˆæ³¨åŠ›ä¸‹é™ã€æ·±åº¦æ€è€ƒæ¸›å¼±ã€‚\n",
      "- æ•¸ä½æˆç™®ï¼šéåº¦åˆ†å¿ƒã€æ‹–å»¶è¡Œç‚ºã€‚\n",
      "- åª’é«”éè¼‰ï¼šå¿½ç•¥æ·±åº¦å…§å®¹ã€ä¾è³´çŸ­æš«åˆºæ¿€ã€‚\n",
      "\n",
      "è¦æ±‚ï¼š\n",
      "1. ç”¢ç”Ÿ 10 å€‹é™³è¿°é¡Œé …ï¼Œæ¯é¡Œä»¥ç¬¬ä¸€äººç¨±æ’°å¯«ï¼ˆå¦‚ã€Œæˆ‘ç¶“å¸¸ç„¡æ³•é›†ä¸­é–±è®€é•·æ–‡ã€ï¼‰ã€‚\n",
      "2. ä½¿ç”¨ Likert é‡è¡¨é¡Œç›®ã€‚\n",
      "3. ç¢ºä¿é¡Œé …å¯é ã€å¤šé¢å‘ï¼Œæ¶µè“‹æ—¥å¸¸æƒ…å¢ƒï¼ˆå¦‚æ»‘æ‰‹æ©Ÿã€Doomscrollingï¼‰ã€‚\n",
      "4. é¿å…é›™é‡å¦å®šå’Œæ¨¡ç³Šèªè¨€ï¼Œç¢ºä¿é¡Œç›®æ¸…æ™°æ˜“æ‡‚ã€‚\n",
      "\n",
      "ğŸ”¢ ç›®æ¨™é¡Œç›®æ•¸é‡: 6\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ”„ ç¬¬ 1 æ¬¡è¿­ä»£\n",
      "ğŸ“ ç”Ÿæˆåˆå§‹é¡Œç›®...\n",
      "âœ… å·²ç”Ÿæˆ 6 å€‹é¡Œç›®\n",
      "é¡Œç›®å…§å®¹: [{'item': 'æˆ‘ç¶“å¸¸ç„¡æ³•é›†ä¸­é–±è®€é•·æ–‡ï¼Œå®¹æ˜“è¢«æ‰‹æ©Ÿé€šçŸ¥å¹²æ“¾ã€‚', 'psychological_construct': 'èªçŸ¥è¡°é€€'}, {'item': 'æˆ‘æœƒç„¡æ„è­˜åœ°åˆ·çŸ­è§†é¢‘ï¼Œå³ä½¿çŸ¥é“é€™æœƒæµªè²»æ™‚é–“ã€‚', 'psychological_construct': 'æ•¸ä½æˆç™®'}, {'item': 'æˆ‘å‚¾å‘æ–¼å¿½ç•¥æ·±åº¦æ–‡ç« ï¼Œå¯§é¡˜ç€è¦½å¤§é‡çŸ­ç¶²ç´…å½±ç‰‡ã€‚', 'psychological_construct': 'åª’é«”éè¼‰'}, {'item': 'æˆ‘è™•ç†å¤šä»»å‹™æ™‚ï¼ˆå¦‚åŒæ™‚çœ‹å½±ç‰‡å’Œå›è¨Šæ¯ï¼‰æ•ˆç‡æ˜é¡¯é™ä½ã€‚', 'psychological_construct': 'èªçŸ¥è¡°é€€'}, {'item': 'æˆ‘åœ¨ç¤¾äº¤åª’é«”ä¸Šç€è¦½æ™‚ï¼Œå¸¸å› éåº¦åˆ†å¿ƒè€Œç„¡æ³•å®ŒæˆåŸå®šä»»å‹™ã€‚', 'psychological_construct': 'æ•¸ä½æˆç™®'}, {'item': 'æˆ‘é›£ä»¥å°ˆæ³¨æ–¼éœ€è¦æ·±åº¦æ€è€ƒçš„ä»»å‹™ï¼Œå®¹æ˜“è¢«å³æ™‚é€šçŸ¥æ‰“æ–­ã€‚', 'psychological_construct': 'èªçŸ¥è¡°é€€'}]\n",
      "ğŸ” è©•å¯©é¡Œç›®å“è³ª...\n",
      "ğŸ“ å…§å®¹æ•ˆåº¦è©•å¯©ä¸­...\n",
      "ğŸ“ èªè¨€å­¸è©•å¯©ä¸­...\n",
      "ğŸ“ åè¦‹æª¢æŸ¥è©•å¯©ä¸­...\n",
      "ğŸ” è©•å¯©çµæœ: \n",
      "                  å…§å®¹æ•ˆåº¦è©•ä¼°ï¼š{'validity_score': 7, 'strengths': ['é¡Œç›®æ¸…æ™°æ˜ç¢ºï¼Œä½¿ç”¨æ—¥å¸¸æƒ…å¢ƒï¼ˆå¦‚æ‰‹æ©Ÿé€šçŸ¥ã€åˆ·çŸ­è§†é¢‘ï¼‰å…·å‚™å¯¦è¸ç›¸é—œæ€§', 'æ¶µè“‹èªçŸ¥è¡°é€€ã€æ•¸ä½æˆç™®ã€åª’é«”éè¼‰ä¸‰å¤§æ ¸å¿ƒæ§‹å¿µï¼Œç¬¦åˆç†è«–æ¡†æ¶', 'æ¡ç”¨ç¬¬ä¸€äººç¨±æ•˜è¿°èˆ‡Likerté‡è¡¨ï¼Œç¬¦åˆå¿ƒç†æ¸¬é©—æ¨™æº–æ ¼å¼'], 'weaknesses': ['åª’é«”éè¼‰æ§‹å¿µåƒ…æœ‰1é¡Œï¼Œç¼ºä¹å°æ·±åº¦è³‡è¨Šè™•ç†èƒ½åŠ›ä¸‹é™çš„æ¸¬é‡', 'æœªæ¶µè“‹æƒ…ç·’å½±éŸ¿ï¼ˆå¦‚ç„¦æ…®ã€æŒ«æ•—æ„Ÿï¼‰èˆ‡ç¤¾äº¤è¡Œç‚ºè®ŠåŒ–çš„ç›¸é—œé¢å‘', 'ç†è«–åŸºç¤å¯åŠ å¼·ï¼Œä¾‹å¦‚æœªæ˜ç¢ºå¼•ç”¨æ³¨æ„åŠ›åˆ†æ•£ç†è«–æˆ–æˆç™®æ¨¡å‹', 'éƒ¨åˆ†é¡Œç›®å­˜åœ¨é‡è¤‡æ€§ï¼ˆå¦‚ã€Œè¢«é€šçŸ¥å¹²æ“¾ã€èˆ‡ã€Œå³æ™‚é€šçŸ¥æ‰“æ–­ã€ï¼‰'], 'suggestions': ['å¢åŠ åª’é«”éè¼‰é¡Œç›®ï¼šä¾‹å¦‚ã€Œæˆ‘é›£ä»¥å°ˆæ³¨é–±è®€éœ€è¦æ·±åº¦æ€è€ƒçš„æ–‡ç« ã€', 'åŠ å…¥æƒ…ç·’ç›¸é—œé¡Œç›®ï¼šä¾‹å¦‚ã€Œç€è¦½çŸ­å½±ç‰‡æ™‚å¸¸æ„Ÿåˆ°ç„¦æ…®æˆ–ä¸å®‰ã€', 'æ˜ç¢ºå¼•ç”¨ç†è«–åŸºç¤ï¼šå¦‚å¼•ç”¨Carrï¼ˆ2010ï¼‰çš„æ³¨æ„åŠ›åˆ†æ•£ç†è«–æˆ–FOMOï¼ˆå®³æ€•éŒ¯éï¼‰æ¦‚å¿µ', 'å„ªåŒ–é¡Œç›®é‡è¤‡æ€§ï¼šå°‡ã€Œè¢«é€šçŸ¥å¹²æ“¾ã€èˆ‡ã€Œå³æ™‚é€šçŸ¥æ‰“æ–­ã€æ•´åˆç‚ºæ›´ç²¾æº–çš„è¡¨è¿°']}\n",
      "                    èªè¨€å­¸è©•ä¼°ï¼š{'readability_score': 8, 'grammar_issues': ['ç¬¬ä¸€é¡Œã€Œå®¹æ˜“è¢«æ‰‹æ©Ÿé€šçŸ¥å¹²æ“¾ã€èªåºå¯æ›´æ¸…æ™°ï¼ˆå»ºè­°ï¼šã€Œå®¹æ˜“è¢«æ‰‹æ©Ÿé€šçŸ¥å¹²æ“¾ã€â†’ã€Œå®¹æ˜“å› æ‰‹æ©Ÿé€šçŸ¥è€Œåˆ†å¿ƒã€ï¼‰', 'ç¬¬å››é¡Œæ‹¬è™Ÿå…§ã€Œå›è¨Šæ¯ã€å»ºè­°è£œå……èªå¢ƒï¼ˆå¦‚ã€Œå›è¦†è¨Šæ¯ã€ï¼‰'], 'clarity_issues': ['ç¬¬ä¸€é¡Œã€Œæ‰‹æ©Ÿé€šçŸ¥å¹²æ“¾ã€å¯èƒ½è¢«èª¤è§£ç‚ºã€Œæ‰‹æ©Ÿæœ¬èº«æ˜¯å¹²æ“¾æºã€ï¼Œå»ºè­°æ˜ç¢ºä¸»èª', 'ç¬¬ä¸‰é¡Œã€ŒçŸ­ç¶²ç´…å½±ç‰‡ã€å¯èƒ½é€ æˆè©èªå †ç–Šï¼ˆå»ºè­°ï¼šã€ŒçŸ­è¦–é »ã€æˆ–ã€Œç¶²ç´…çŸ­å½±ç‰‡ã€ï¼‰'], 'suggestions': ['çµ±ä¸€ä½¿ç”¨ã€Œåˆ†å¿ƒ/å¹²æ“¾ã€ä½œç‚ºæ ¸å¿ƒå‹•è©ï¼Œé¿å…æ··ç”¨ã€Œå¹²æ“¾ã€èˆ‡ã€Œåˆ†å¿ƒã€', 'å°‡ã€Œæ•¸ä½æˆç™®ã€èˆ‡ã€ŒèªçŸ¥è¡°é€€ã€çš„è¡¨è¿°å€åˆ†æ›´æ˜é¡¯ï¼ˆä¾‹å¦‚ï¼šå‰è€…å¼·èª¿è¡Œç‚ºï¼Œå¾Œè€…å¼·èª¿èƒ½åŠ›ä¸‹é™ï¼‰', 'åœ¨æ‹¬è™Ÿèªªæ˜ä¸­è£œå……å…·é«”æƒ…å¢ƒï¼ˆå¦‚ã€ŒåŒæ™‚é€²è¡Œå¤šé …ä»»å‹™ã€ï¼‰ï¼Œæå‡é¡Œç›®æƒ…å¢ƒçš„æ˜ç¢ºæ€§']}\n",
      "                    åè¦‹æª¢æŸ¥è©•ä¼°ï¼š{'bias_score': 2, 'detected_biases': [], 'problematic_items': [], 'suggestions': ['è€ƒæ…®å¢åŠ é¡Œç›®ä¸­ç«‹æ€§ï¼Œä¾‹å¦‚å°‡ã€Œæ‰‹æ©Ÿé€šçŸ¥ã€æ”¹ç‚ºã€Œé›»å­è¨­å‚™é€šçŸ¥ã€ä»¥é¿å…éåº¦èšç„¦æ–¼æ‰‹æ©Ÿ', 'å¯åŠ å…¥è·¨å¹´é½¡å±¤çš„è¡Œç‚ºæè¿°ï¼Œä¾‹å¦‚ã€Œè™•ç†å¤šä»»å‹™æ™‚ï¼ˆå¦‚åŒæ™‚é–±è®€æ›¸ç±å’Œå›è¨Šæ¯ï¼‰æ•ˆç‡é™ä½ã€', 'é¿å…ä½¿ç”¨å¯èƒ½èˆ‡ç‰¹å®šæ–‡åŒ–ç¿’æ…£ç›¸é—œçš„è©å½™ï¼Œä¾‹å¦‚ã€Œç¶²ç´…å½±ç‰‡ã€å¯æ”¹ç‚ºã€ŒçŸ­è¦–é »å…§å®¹ã€']}\n",
      "                    å…ƒè©•å¯©çµæœï¼š{'overall_score': 6, 'strengths': ['é¡Œç›®æƒ…å¢ƒè²¼è¿‘æ—¥å¸¸æ•¸ä½ä½¿ç”¨è¡Œç‚ºï¼Œå…·å‚™å¯¦è¸ç›¸é—œæ€§', 'æˆåŠŸå€åˆ†ã€Œæ•¸ä½æˆç™®ã€èˆ‡ã€ŒèªçŸ¥è¡°é€€ã€å…©å¤§æ ¸å¿ƒæ§‹å¿µ', 'æ¡ç”¨ç¬¬ä¸€äººç¨±æ•˜è¿°èˆ‡æ¨™æº–å¿ƒç†æ¸¬é©—æ ¼å¼'], 'major_issues': ['åª’é«”éè¼‰æ¸¬é‡ä¸è¶³ä¸”å­˜åœ¨é¡Œç›®é‡è¤‡ï¼ˆ3é¡ŒæåŠåˆ†å¿ƒ/å¹²æ“¾ï¼‰', 'ç¼ºä¹æƒ…ç·’å½±éŸ¿èˆ‡ç¤¾äº¤è¡Œç‚ºè®ŠåŒ–çš„æ¸¬é‡ç¶­åº¦', 'ç†è«–åŸºç¤æœªæ˜ç¢ºå¼•ç”¨ï¼Œå½±éŸ¿å…§å®¹æ•ˆåº¦', 'èªè¨€è¡¨è¿°å­˜åœ¨èªåºèˆ‡è©èªå †ç–Šå•é¡Œ', 'æ½›åœ¨æ–‡åŒ–åè¦‹ï¼ˆå¦‚éåº¦èšç„¦æ‰‹æ©Ÿèˆ‡ç¶²ç´…å½±ç‰‡ï¼‰'], 'recommendations': ['æ–°å¢åª’é«”éè¼‰é¡Œç›®ï¼šä¾‹å¦‚ã€Œæˆ‘é›£ä»¥å°ˆæ³¨é–±è®€éœ€è¦æ·±åº¦æ€è€ƒçš„æ–‡ç« ã€', 'åŠ å…¥æƒ…ç·’ç›¸é—œé¡Œç›®ï¼šä¾‹å¦‚ã€Œç€è¦½çŸ­å½±ç‰‡æ™‚å¸¸æ„Ÿåˆ°ç„¦æ…®æˆ–ä¸å®‰ã€', 'å¼•ç”¨æ³¨æ„åŠ›åˆ†æ•£ç†è«–æˆ–FOMOæ¦‚å¿µå¼·åŒ–ç†è«–åŸºç¤', 'æ•´åˆé‡è¤‡æ€§é¡Œç›®ï¼ˆå¦‚å°‡ã€Œæ‰‹æ©Ÿé€šçŸ¥å¹²æ“¾ã€èˆ‡ã€Œå³æ™‚é€šçŸ¥æ‰“æ–­ã€åˆä½µç‚ºã€Œå®¹æ˜“å› é›»å­è¨­å‚™é€šçŸ¥è€Œåˆ†å¿ƒã€ï¼‰', 'çµ±ä¸€ä½¿ç”¨ã€Œåˆ†å¿ƒã€ä½œç‚ºæ ¸å¿ƒå‹•è©ï¼Œå€åˆ†ã€Œæ•¸ä½æˆç™®ã€ï¼ˆè¡Œç‚ºï¼‰èˆ‡ã€ŒèªçŸ¥è¡°é€€ã€ï¼ˆèƒ½åŠ›ä¸‹é™ï¼‰', 'å°‡ã€Œç¶²ç´…å½±ç‰‡ã€æ”¹ç‚ºã€ŒçŸ­è¦–é »å…§å®¹ã€ï¼Œå¢åŠ ã€Œé›»å­è¨­å‚™é€šçŸ¥ã€çš„ä¸­ç«‹è¡¨è¿°'], 'regenerate_recommended': True}\n",
      "ğŸ“Š ç¶œåˆè©•åˆ†: 6/10\n",
      "âš ï¸ éœ€è¦ç¹¼çºŒæ”¹é€²\n",
      "\n",
      "ğŸ”„ ç¬¬ 2 æ¬¡è¿­ä»£\n",
      "ğŸ”§ æ ¹æ“šè©•å¯©å»ºè­°æ”¹é€²é¡Œç›®...\n",
      "âœ… å·²ç”Ÿæˆ 8 å€‹é¡Œç›®\n",
      "é¡Œç›®å…§å®¹: [{'item': 'æˆ‘å®¹æ˜“å› é›»å­è¨­å‚™é€šçŸ¥è€Œåˆ†å¿ƒï¼Œé›£ä»¥æŒçºŒå°ˆæ³¨æ–¼æ·±åº¦æ€è€ƒä»»å‹™ã€‚', 'psychological_construct': 'æ³¨æ„åŠ›åˆ†æ•£'}, {'item': 'æˆ‘æœƒç„¡æ„è­˜åœ°åˆ·çŸ­è§†é¢‘ï¼Œå³ä½¿çŸ¥é“é€™æœƒæµªè²»æ™‚é–“ï¼Œä¸”å¸¸ä¼´éš¨ç„¦æ…®æƒ…ç·’ã€‚', 'psychological_construct': 'æ•¸ä½æˆç™®'}, {'item': 'æˆ‘é›£ä»¥å°ˆæ³¨é–±è®€éœ€è¦æ·±åº¦æ€è€ƒçš„æ–‡ç« ï¼Œå®¹æ˜“è¢«çŸ­è¦–é »å…§å®¹å¸å¼•ã€‚', 'psychological_construct': 'åª’é«”éè¼‰'}, {'item': 'è™•ç†å¤šä»»å‹™æ™‚ï¼ˆå¦‚åŒæ™‚çœ‹å½±ç‰‡å’Œå›è¨Šæ¯ï¼‰æ•ˆç‡æ˜é¡¯é™ä½ï¼Œå¸¸æ„Ÿåˆ°å£“åŠ›å¢å¤§ã€‚', 'psychological_construct': 'èªçŸ¥è¡°é€€'}, {'item': 'ç€è¦½ç¤¾äº¤åª’é«”æ™‚ï¼Œå› éåº¦åˆ†å¿ƒè€Œç„¡æ³•å®ŒæˆåŸå®šä»»å‹™ï¼Œå¶æœ‰éŒ¯å¤±ææ‡¼æ„Ÿï¼ˆFOMOï¼‰ã€‚', 'psychological_construct': 'æ•¸ä½æˆç™®'}, {'item': 'æˆ‘é›£ä»¥å°ˆæ³¨æ–¼éœ€è¦æ·±åº¦æ€è€ƒçš„ä»»å‹™ï¼Œå®¹æ˜“è¢«å³æ™‚é€šçŸ¥æ‰“æ–­ï¼Œå°è‡´æ•ˆç‡ä¸‹é™ã€‚', 'psychological_construct': 'èªçŸ¥è¡°é€€'}, {'item': 'ç€è¦½çŸ­å½±ç‰‡æ™‚å¸¸æ„Ÿåˆ°ç„¦æ…®æˆ–ä¸å®‰ï¼Œæ“”å¿ƒéŒ¯éé‡è¦è³‡è¨Šæˆ–ç¤¾äº¤äº’å‹•ã€‚', 'psychological_construct': 'FOMOç¾è±¡'}, {'item': 'æˆ‘å‚¾å‘æ–¼å¿«é€Ÿç€è¦½å¤§é‡çŸ­è¦–é »å…§å®¹ï¼Œè€Œéæ·±å…¥é–±è®€é•·ç¯‡æ–‡å­—è³‡æ–™ã€‚', 'psychological_construct': 'åª’é«”éè¼‰'}]\n",
      "ğŸ” è©•å¯©é¡Œç›®å“è³ª...\n",
      "ğŸ“ å…§å®¹æ•ˆåº¦è©•å¯©ä¸­...\n",
      "ğŸ“ èªè¨€å­¸è©•å¯©ä¸­...\n",
      "ğŸ“ åè¦‹æª¢æŸ¥è©•å¯©ä¸­...\n",
      "ğŸ” è©•å¯©çµæœ: \n",
      "                  å…§å®¹æ•ˆåº¦è©•ä¼°ï¼š{'validity_score': 8, 'strengths': ['é¡Œé …æ¸…æ¥šå€åˆ†ä¸‰å¤§æ ¸å¿ƒæ§‹å¿µï¼ˆèªçŸ¥è¡°é€€/æ•¸ä½æˆç™®/åª’é«”éè¼‰ï¼‰ï¼Œç¬¦åˆå¿ƒç†æ¸¬é©—çš„å…§å®¹æ•ˆåº¦è¦æ±‚', 'æ¶µè“‹æ—¥å¸¸æ•¸ä½ä½¿ç”¨æƒ…å¢ƒï¼ˆå¦‚æ»‘æ‰‹æ©Ÿã€Doomscrollingï¼‰ï¼Œæå‡é¡Œé …çš„ç¾å¯¦é—œè¯æ€§', 'ä½¿ç”¨ç¬¬ä¸€äººç¨±æ•˜è¿°èˆ‡Likerté‡è¡¨ï¼Œç¬¦åˆè‡ªæˆ‘å ±å‘Šæ¸¬é©—çš„å¸¸è¦è¨­è¨ˆ', 'é¡Œé …é–“å­˜åœ¨ä¸€å®šç¨‹åº¦çš„å€åˆ¥æ•ˆåº¦ï¼ˆå¦‚å°‡FOMOç¾è±¡å–®ç¨æ­¸é¡æ–¼æ•¸ä½æˆç™®ï¼‰'], 'weaknesses': ['FOMOç¾è±¡æœªæ˜ç¢ºç´å…¥åŸå§‹å¿ƒç†å»ºæ§‹å®šç¾©ï¼Œå¯èƒ½é€ æˆæ¦‚å¿µæ··æ·†', 'ã€ŒèªçŸ¥è¡°é€€ã€é¡Œé …é‡è¤‡åº¦è¼ƒé«˜ï¼ˆå¦‚ç¬¬4ã€6é¡Œå‡æ¶‰åŠå¤šä»»å‹™è™•ç†æ•ˆç‡ä¸‹é™ï¼‰', 'ç¼ºä¹æ¸¬é‡ã€Œæ·±åº¦æ€è€ƒèƒ½åŠ›ä¸‹é™ã€çš„ç›´æ¥æŒ‡æ¨™ï¼ˆå¦‚ç„¡æ³•é€²è¡ŒæŠ½è±¡æ¨ç†æˆ–æ‰¹åˆ¤æ€§æ€è€ƒï¼‰', 'ã€Œåª’é«”éè¼‰ã€é¡Œé …ä¸»è¦å´é‡è¡Œç‚ºè¡¨ç¾ï¼Œæœªæ¶µè“‹èªçŸ¥å±¤é¢çš„è³‡è¨Šè™•ç†èƒ½åŠ›è©•ä¼°'], 'suggestions': ['å°‡FOMOç¾è±¡é‡æ–°æ­¸é¡è‡³ã€Œæ•¸ä½æˆç™®ã€æˆ–ä½œç‚ºé™„åŠ æ¬¡ç´šæ§‹å¿µï¼Œä¸¦æ˜ç¢ºç•Œå®šå…¶èˆ‡ä¸»æ§‹å¿µçš„é—œä¿‚', 'å¢åŠ æ¸¬é‡æ·±åº¦æ€è€ƒèƒ½åŠ›çš„é¡Œé …ï¼ˆå¦‚ã€Œæˆ‘é›£ä»¥ç†è§£éœ€è¦é‚è¼¯é€£è²«çš„é•·ç¯‡æ–‡ç« ã€ï¼‰', 'è£œå……é—œæ–¼æ•¸ä½å…§å®¹å“è³ªå½±éŸ¿çš„é¡Œé …ï¼ˆå¦‚ã€Œæˆ‘æœƒå› ç‚ºçŸ­å½±ç‰‡çš„ç¢ç‰‡åŒ–æ•˜äº‹è€Œé›£ä»¥å½¢æˆå®Œæ•´ç†è§£ã€ï¼‰', 'å¢åŠ åå‘é¡Œé …ä»¥æå‡é‡è¡¨çš„å…§éƒ¨ä¸€è‡´æ€§ï¼ˆå¦‚ã€Œæˆ‘èƒ½å¤ åœ¨ä¸å—å¹²æ“¾çš„æƒ…å¢ƒä¸‹å®Œæˆæ·±åº¦æ€è€ƒä»»å‹™ã€ï¼‰']}\n",
      "                    èªè¨€å­¸è©•ä¼°ï¼š{'readability_score': 8, 'grammar_issues': [], 'clarity_issues': ['ã€Œæ·±åº¦æ€è€ƒä»»å‹™ã€å»ºè­°èª¿æ•´ç‚ºã€Œéœ€è¦æ·±åº¦æ€è€ƒçš„ä»»å‹™ã€ä»¥æé«˜èªæ„æ˜ç¢ºæ€§', 'ã€ŒçŸ­è¦–é »å…§å®¹ã€å»ºè­°çµ±ä¸€ç‚ºã€ŒçŸ­è§†é¢‘å…§å®¹ã€ä»¥ç¬¦åˆå¸¸è¦‹ç”¨èª', 'ã€ŒFOMOç¾è±¡ã€å»ºè­°è£œå……èªªæ˜ç‚ºã€ŒéŒ¯å¤±ææ‡¼ç—‡ï¼ˆFOM/NFOMOï¼‰ã€ä»¥é¿å…æ¦‚å¿µæ··æ·†', 'ã€ŒèªçŸ¥è¡°é€€ã€å»ºè­°èª¿æ•´ç‚ºã€ŒèªçŸ¥éè¼‰ã€ä»¥ç¬¦åˆå¿ƒç†å­¸å¸¸ç”¨è¡“èª', 'ã€Œåª’é«”éè¼‰ã€å»ºè­°æ˜ç¢ºåŒ–ç‚ºã€Œè³‡è¨Šéè¼‰ã€ä»¥é¿å…èˆ‡åª’é«”é¡å‹æ··æ·†'], 'suggestions': ['çµ±ä¸€ä½¿ç”¨ã€Œæ•¸ä½æˆç™®ã€è€Œéã€Œæ•¸ä½æˆç™®ã€çš„ä¸­è‹±æ–‡æ··ç”¨', 'å°‡ã€ŒFOMOã€ä»¥æ‹¬è™Ÿè£œå……èªªæ˜ç‚ºã€ŒéŒ¯å¤±ææ‡¼ç—‡ï¼ˆFOMOï¼‰ã€', 'èª¿æ•´ã€Œè™•ç†å¤šä»»å‹™æ™‚ã€ç‚ºã€Œåœ¨è™•ç†å¤šä»»å‹™æ™‚ã€ä½¿èªæ°£æ›´è‡ªç„¶', 'å°‡ã€Œé•·ç¯‡æ–‡å­—è³‡æ–™ã€æ”¹ç‚ºã€Œé•·ç¯‡æ–‡å­—ææ–™ã€ä»¥ç¬¦åˆå­¸è¡“ç”¨èª', 'åœ¨ã€Œæ•¸ä½æˆç™®ã€ç›¸é—œé¡Œç›®ä¸­å¢åŠ è¡Œç‚ºå‹•æ©Ÿæè¿°ï¼ˆå¦‚ã€Œç„¡æ³•è‡ªæ§ã€ï¼‰ä»¥å¼·åŒ–æ¸¬é©—æ•ˆåº¦']}\n",
      "                    åè¦‹æª¢æŸ¥è©•ä¼°ï¼š{'bias_score': 10, 'detected_biases': [], 'problematic_items': [], 'suggestions': ['æ‰€æœ‰é¡Œç›®å‡æœªç™¼ç¾æ˜é¡¯äººå£å­¸åè¦‹ï¼Œå»ºè­°ä¿æŒç¾ç‹€ã€‚è‹¥éœ€é€²ä¸€æ­¥ç²¾ç·»åŒ–ï¼Œå¯å¢åŠ ä¸­æ€§åŒ–æ•˜è¿°ï¼ˆä¾‹å¦‚å°‡ã€ŒçŸ­å½±ç‰‡ã€æ”¹ç‚ºã€Œè¦–é »å…§å®¹ã€ï¼‰ï¼Œä»¥é™ä½èˆ‡ç‰¹å®šæ–‡åŒ–æˆ–å¹´é½¡å±¤çš„é—œè¯æ€§ã€‚']}\n",
      "                    å…ƒè©•å¯©çµæœï¼š{'overall_score': 7, 'strengths': ['é¡Œé …æ¸…æ™°å€åˆ†ä¸‰å¤§æ ¸å¿ƒæ§‹å¿µï¼ˆèªçŸ¥è¡°é€€/æ•¸ä½æˆç™®/åª’é«”éè¼‰ï¼‰', 'æ¶µè“‹æ—¥å¸¸æ•¸ä½ä½¿ç”¨æƒ…å¢ƒï¼ˆå¦‚æ»‘æ‰‹æ©Ÿã€Doomscrollingï¼‰', 'ä½¿ç”¨ç¬¬ä¸€äººç¨±æ•˜è¿°èˆ‡Likerté‡è¡¨ç¬¦åˆæ¸¬é©—è¨­è¨ˆå¸¸è¦', 'èªè¨€è¡¨é”æ•´é«”æ¸…æ™°ï¼Œç„¡æ˜é¡¯èªæ³•éŒ¯èª¤', 'æœªç™¼ç¾æ˜é¡¯äººå£å­¸åè¦‹'], 'major_issues': ['FOMOç¾è±¡æ¦‚å¿µç•Œå®šä¸æ¸…æ™°ï¼Œèˆ‡æ•¸ä½æˆç™®çš„å€åˆ†éœ€æ˜ç¢º', 'ã€ŒèªçŸ¥è¡°é€€ã€é¡Œé …é‡è¤‡åº¦é«˜ï¼ˆç¬¬4ã€6é¡Œï¼‰', 'ç¼ºä¹æ¸¬é‡æ·±åº¦æ€è€ƒèƒ½åŠ›çš„ç›´æ¥æŒ‡æ¨™', 'ã€Œåª’é«”éè¼‰ã€æœªæ¶µè“‹èªçŸ¥å±¤é¢è³‡è¨Šè™•ç†èƒ½åŠ›', 'è¡“èªä½¿ç”¨ä¸çµ±ä¸€ï¼ˆå¦‚ã€ŒçŸ­å½±ç‰‡ã€vsã€ŒçŸ­è§†é¢‘ã€ï¼‰'], 'recommendations': ['å°‡FOMOç¾è±¡æ˜ç¢ºæ­¸é¡è‡³æ•¸ä½æˆç™®ï¼Œä¸¦è£œå……FOMOçš„å®šç¾©èªªæ˜', 'åˆªé™¤é‡è¤‡çš„èªçŸ¥è¡°é€€é¡Œé …ï¼ˆå¦‚ç¬¬4ã€6é¡Œï¼‰ï¼Œå¢åŠ æ–°é¡Œé …æ¸¬é‡æ·±åº¦æ€è€ƒèƒ½åŠ›', 'çµ±ä¸€è¡“èªç‚ºã€ŒçŸ­è§†é¢‘å…§å®¹ã€ä¸¦è£œå……ã€Œè³‡è¨Šéè¼‰ã€çš„æ˜ç¢ºå®šç¾©', 'å¢åŠ åå‘é¡Œé …ï¼ˆå¦‚ã€Œæˆ‘èƒ½å¤ åœ¨ä¸å—å¹²æ“¾çš„æƒ…å¢ƒä¸‹å®Œæˆæ·±åº¦æ€è€ƒä»»å‹™ã€ï¼‰', 'è£œå……æ•¸ä½å…§å®¹å“è³ªå½±éŸ¿çš„é¡Œé …ï¼ˆå¦‚ç¢ç‰‡åŒ–æ•˜äº‹å°ç†è§£çš„å½±éŸ¿ï¼‰'], 'regenerate_recommended': True}\n",
      "ğŸ“Š ç¶œåˆè©•åˆ†: 7/10\n",
      "âœ… é¡Œç›®å“è³ªå·²é”æ¨™æº–ï¼ŒçµæŸè¿­ä»£\n",
      "\n",
      "ğŸ“Š é€²è¡Œå¿ƒç†è¨ˆé‡åˆ†æ...\n",
      "âœ… åˆ†æå®Œæˆ\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ LM-AIG ç³»çµ±åŸ·è¡Œçµæœ\n",
      "============================================================\n",
      "\n",
      "ğŸ“ åŸå§‹è¦æ ¼: \n",
      "è«‹è¨­è¨ˆä¸€å€‹ã€ŒBrainrot å½±éŸ¿ç¨‹åº¦é‡è¡¨ã€ï¼ˆBrainrot Impact Scaleï¼‰ï¼Œç”¨æ–¼è©•ä¼°å€‹äººå› é•·æœŸæ¥è§¸ä½å“è³ªæ•¸ä½å…§å®¹ï¼ˆå¦‚çŸ­å½±ç‰‡ã€ç¢ç‰‡è³‡è¨Šï¼‰è€Œç”¢ç”Ÿçš„è…¦è…å½±éŸ¿ã€‚\n",
      "\n",
      "æ ¸å¿ƒæ§‹å¿µï¼š\n",
      "- èªçŸ¥è¡°é€€ï¼šå°ˆæ³¨åŠ›ä¸‹é™ã€æ·±åº¦æ€è€ƒæ¸›å¼±ã€‚\n",
      "- æ•¸ä½æˆç™®ï¼šéåº¦åˆ†å¿ƒã€æ‹–å»¶è¡Œç‚ºã€‚\n",
      "- åª’é«”éè¼‰ï¼šå¿½ç•¥æ·±åº¦å…§å®¹ã€ä¾è³´çŸ­æš«åˆºæ¿€ã€‚\n",
      "\n",
      "è¦æ±‚ï¼š\n",
      "1. ç”¢ç”Ÿ 10 å€‹é™³è¿°é¡Œé …ï¼Œæ¯é¡Œä»¥ç¬¬ä¸€äººç¨±æ’°å¯«ï¼ˆå¦‚ã€Œæˆ‘ç¶“å¸¸ç„¡æ³•é›†ä¸­é–±è®€é•·æ–‡ã€ï¼‰ã€‚\n",
      "2. ä½¿ç”¨ Likert é‡è¡¨é¡Œç›®ã€‚\n",
      "3. ç¢ºä¿é¡Œé …å¯é ã€å¤šé¢å‘ï¼Œæ¶µè“‹æ—¥å¸¸æƒ…å¢ƒï¼ˆå¦‚æ»‘æ‰‹æ©Ÿã€Doomscrollingï¼‰ã€‚\n",
      "4. é¿å…é›™é‡å¦å®šå’Œæ¨¡ç³Šèªè¨€ï¼Œç¢ºä¿é¡Œç›®æ¸…æ™°æ˜“æ‡‚ã€‚\n",
      "\n",
      "ğŸ”„ è¿­ä»£æ¬¡æ•¸: 2\n",
      "\n",
      "ğŸ“Š æœ€çµ‚é¡Œç›® (8 å€‹):\n",
      " 1. {'item': 'æˆ‘å®¹æ˜“å› é›»å­è¨­å‚™é€šçŸ¥è€Œåˆ†å¿ƒï¼Œé›£ä»¥æŒçºŒå°ˆæ³¨æ–¼æ·±åº¦æ€è€ƒä»»å‹™ã€‚', 'psychological_construct': 'æ³¨æ„åŠ›åˆ†æ•£'}\n",
      " 2. {'item': 'æˆ‘æœƒç„¡æ„è­˜åœ°åˆ·çŸ­è§†é¢‘ï¼Œå³ä½¿çŸ¥é“é€™æœƒæµªè²»æ™‚é–“ï¼Œä¸”å¸¸ä¼´éš¨ç„¦æ…®æƒ…ç·’ã€‚', 'psychological_construct': 'æ•¸ä½æˆç™®'}\n",
      " 3. {'item': 'æˆ‘é›£ä»¥å°ˆæ³¨é–±è®€éœ€è¦æ·±åº¦æ€è€ƒçš„æ–‡ç« ï¼Œå®¹æ˜“è¢«çŸ­è¦–é »å…§å®¹å¸å¼•ã€‚', 'psychological_construct': 'åª’é«”éè¼‰'}\n",
      " 4. {'item': 'è™•ç†å¤šä»»å‹™æ™‚ï¼ˆå¦‚åŒæ™‚çœ‹å½±ç‰‡å’Œå›è¨Šæ¯ï¼‰æ•ˆç‡æ˜é¡¯é™ä½ï¼Œå¸¸æ„Ÿåˆ°å£“åŠ›å¢å¤§ã€‚', 'psychological_construct': 'èªçŸ¥è¡°é€€'}\n",
      " 5. {'item': 'ç€è¦½ç¤¾äº¤åª’é«”æ™‚ï¼Œå› éåº¦åˆ†å¿ƒè€Œç„¡æ³•å®ŒæˆåŸå®šä»»å‹™ï¼Œå¶æœ‰éŒ¯å¤±ææ‡¼æ„Ÿï¼ˆFOMOï¼‰ã€‚', 'psychological_construct': 'æ•¸ä½æˆç™®'}\n",
      " 6. {'item': 'æˆ‘é›£ä»¥å°ˆæ³¨æ–¼éœ€è¦æ·±åº¦æ€è€ƒçš„ä»»å‹™ï¼Œå®¹æ˜“è¢«å³æ™‚é€šçŸ¥æ‰“æ–­ï¼Œå°è‡´æ•ˆç‡ä¸‹é™ã€‚', 'psychological_construct': 'èªçŸ¥è¡°é€€'}\n",
      " 7. {'item': 'ç€è¦½çŸ­å½±ç‰‡æ™‚å¸¸æ„Ÿåˆ°ç„¦æ…®æˆ–ä¸å®‰ï¼Œæ“”å¿ƒéŒ¯éé‡è¦è³‡è¨Šæˆ–ç¤¾äº¤äº’å‹•ã€‚', 'psychological_construct': 'FOMOç¾è±¡'}\n",
      " 8. {'item': 'æˆ‘å‚¾å‘æ–¼å¿«é€Ÿç€è¦½å¤§é‡çŸ­è¦–é »å…§å®¹ï¼Œè€Œéæ·±å…¥é–±è®€é•·ç¯‡æ–‡å­—è³‡æ–™ã€‚', 'psychological_construct': 'åª’é«”éè¼‰'}\n",
      "\n",
      "ğŸ“ˆ è©•åˆ†æ­·ç¨‹:\n",
      "  ç¬¬ 1 æ¬¡è¿­ä»£: 6/10\n",
      "  ç¬¬ 2 æ¬¡è¿­ä»£: 7/10\n",
      "\n",
      "========================================\n",
      "ğŸ“Š å¿ƒç†è¨ˆé‡åˆ†æçµæœ\n",
      "========================================\n",
      "\n",
      "ğŸ“Š æ¸¬é©—å¿ƒç†è¨ˆé‡åˆ†æå ±å‘Š\n",
      "==================================================\n",
      "\n",
      "ğŸ”¢ åŸºæœ¬è³‡è¨Š:\n",
      "- æ¨£æœ¬æ•¸: 200\n",
      "- é¡Œç›®æ•¸: 8\n",
      "- å› å­æ•¸: 1\n",
      "\n",
      "ğŸ“ˆ ä¿¡åº¦åˆ†æ:\n",
      "- Cronbach's Alpha: 0.925\n",
      "\n",
      "ğŸ¯ å› å­åˆ†æçµæœ:\n",
      "è§£é‡‹è®Šç•°é‡æ¯”ä¾‹: ['1.000']\n",
      "\n",
      "ğŸ“‹ å› å­è² è·é‡:\n",
      "        Factor_1\n",
      "Item_1    -0.811\n",
      "Item_2    -0.802\n",
      "Item_3    -0.802\n",
      "Item_4    -0.809\n",
      "Item_5    -0.680\n",
      "Item_6    -0.674\n",
      "Item_7    -0.822\n",
      "Item_8    -0.836\n",
      "\n",
      "ğŸ“Š é¡Œç›®çµ±è¨ˆ:\n",
      "\n",
      "Item_1:  å¹³å‡æ•¸: 4.36  æ¨™æº–å·®: 0.66  é …ç›®-ç¸½åˆ†ç›¸é—œ: 0.775\n",
      "Item_2:  å¹³å‡æ•¸: 4.52  æ¨™æº–å·®: 0.60  é …ç›®-ç¸½åˆ†ç›¸é—œ: 0.769\n",
      "Item_3:  å¹³å‡æ•¸: 3.04  æ¨™æº–å·®: 0.73  é …ç›®-ç¸½åˆ†ç›¸é—œ: 0.766\n",
      "Item_4:  å¹³å‡æ•¸: 3.04  æ¨™æº–å·®: 0.73  é …ç›®-ç¸½åˆ†ç›¸é—œ: 0.780\n",
      "Item_5:  å¹³å‡æ•¸: 4.14  æ¨™æº–å·®: 0.60  é …ç›®-ç¸½åˆ†ç›¸é—œ: 0.656\n",
      "Item_6:  å¹³å‡æ•¸: 3.62  æ¨™æº–å·®: 0.55  é …ç›®-ç¸½åˆ†ç›¸é—œ: 0.648\n",
      "Item_7:  å¹³å‡æ•¸: 3.75  æ¨™æº–å·®: 0.72  é …ç›®-ç¸½åˆ†ç›¸é—œ: 0.789\n",
      "Item_8:  å¹³å‡æ•¸: 4.12  æ¨™æº–å·®: 0.71  é …ç›®-ç¸½åˆ†ç›¸é—œ: 0.795\n",
      "\n",
      "ğŸ’¡ è©•ä¼°å»ºè­°:\n",
      "âœ… ä¿¡åº¦è‰¯å¥½ (Î± â‰¥ 0.8)\n",
      "âœ… æ‰€æœ‰é¡Œç›®èˆ‡ç¸½åˆ†ç›¸é—œè‰¯å¥½\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# å®šç¾©æ¸¬é©—è¦æ ¼\n",
    "specifications = \"\"\"\n",
    "è«‹è¨­è¨ˆä¸€å€‹ã€ŒBrainrot å½±éŸ¿ç¨‹åº¦é‡è¡¨ã€ï¼ˆBrainrot Impact Scaleï¼‰ï¼Œç”¨æ–¼è©•ä¼°å€‹äººå› é•·æœŸæ¥è§¸ä½å“è³ªæ•¸ä½å…§å®¹ï¼ˆå¦‚çŸ­å½±ç‰‡ã€ç¢ç‰‡è³‡è¨Šï¼‰è€Œç”¢ç”Ÿçš„è…¦è…å½±éŸ¿ã€‚\n",
    "\n",
    "æ ¸å¿ƒæ§‹å¿µï¼š\n",
    "- èªçŸ¥è¡°é€€ï¼šå°ˆæ³¨åŠ›ä¸‹é™ã€æ·±åº¦æ€è€ƒæ¸›å¼±ã€‚\n",
    "- æ•¸ä½æˆç™®ï¼šéåº¦åˆ†å¿ƒã€æ‹–å»¶è¡Œç‚ºã€‚\n",
    "- åª’é«”éè¼‰ï¼šå¿½ç•¥æ·±åº¦å…§å®¹ã€ä¾è³´çŸ­æš«åˆºæ¿€ã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. ç”¢ç”Ÿ 10 å€‹é™³è¿°é¡Œé …ï¼Œæ¯é¡Œä»¥ç¬¬ä¸€äººç¨±æ’°å¯«ï¼ˆå¦‚ã€Œæˆ‘ç¶“å¸¸ç„¡æ³•é›†ä¸­é–±è®€é•·æ–‡ã€ï¼‰ã€‚\n",
    "2. ä½¿ç”¨ Likert é‡è¡¨é¡Œç›®ã€‚\n",
    "3. ç¢ºä¿é¡Œé …å¯é ã€å¤šé¢å‘ï¼Œæ¶µè“‹æ—¥å¸¸æƒ…å¢ƒï¼ˆå¦‚æ»‘æ‰‹æ©Ÿã€Doomscrollingï¼‰ã€‚\n",
    "4. é¿å…é›™é‡å¦å®šå’Œæ¨¡ç³Šèªè¨€ï¼Œç¢ºä¿é¡Œç›®æ¸…æ™°æ˜“æ‡‚ã€‚\n",
    "\"\"\"\n",
    "num_items = 6\n",
    "max_iterations = 3\n",
    "\n",
    "print(\"ğŸ¯ é–‹å§‹æ¼”ç¤ºï¼šBrainrot å½±éŸ¿ç¨‹åº¦é‡è¡¨ç”Ÿæˆ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# é‹è¡Œå®Œæ•´å·¥ä½œæµç¨‹\n",
    "demo_results = lm_aig_system.run_complete_workflow(\n",
    "    specifications=specifications,\n",
    "    num_items=num_items,\n",
    "    max_iterations=max_iterations\n",
    ")\n",
    "\n",
    "# é¡¯ç¤ºçµæœ\n",
    "lm_aig_system.display_results(demo_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å–®ç¨çµ„ä»¶ä½¿ç”¨ç¤ºä¾‹\n",
    "\n",
    "æ‚¨ä¹Ÿå¯ä»¥å–®ç¨ä½¿ç”¨å„å€‹çµ„ä»¶é€²è¡Œæ¸¬è©¦ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹ 1: å–®ç¨ä½¿ç”¨é¡Œç›®ç”Ÿæˆå™¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ç¤ºä¾‹ 1: å–®ç¨ç”Ÿæˆé¡Œç›®\n",
      "------------------------------\n",
      "ç”Ÿæˆçµæœ:\n",
      " 1. åœ¨éå»ä¸€å€‹æœˆè£¡ï¼Œæ‚¨æ˜¯å¦è¦ºå¾—è‡ªå·±çš„ç”Ÿæ´»æ•´é«”ä¾†èªªæ˜¯æ»¿æ„çš„ï¼Ÿ\n",
      " 2. ç•¶æ‚¨å›æƒ³éå»çš„ç¶“æ­·æ™‚ï¼Œæ˜¯å¦ç¶“å¸¸æ„Ÿåˆ°è‡ªå·±çš„ç”Ÿæ´»æœ‰åƒ¹å€¼ä¸”æœ‰æ„ç¾©ï¼Ÿ\n",
      " 3. åœ¨é¢å°æŒ‘æˆ°æ™‚ï¼Œæ‚¨æ˜¯å¦è¦ºå¾—è‡ªå·±æœ‰èƒ½åŠ›å»æ”¹è®Šæˆ–æ”¹å–„ç•¶å‰çš„æƒ…å¢ƒï¼Ÿ\n",
      " 4. æ‚¨æ˜¯å¦è¦ºå¾—è‡ªå·±åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­èƒ½å¤ å¹³è¡¡å·¥ä½œèˆ‡ä¼‘æ¯ï¼Œä¸¦ç¶­æŒè‰¯å¥½çš„èº«å¿ƒç‹€æ…‹ï¼Ÿ\n",
      " 5. ç•¶æ‚¨æ€è€ƒæœªä¾†æ™‚ï¼Œæ˜¯å¦å°è‡ªå·±çš„ç”Ÿæ´»æ–¹å‘å’Œç›®æ¨™æ„Ÿåˆ°æœŸå¾…èˆ‡ä¿¡å¿ƒï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“ ç¤ºä¾‹ 1: å–®ç¨ç”Ÿæˆé¡Œç›®\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "simple_spec = \"\"\"ç”Ÿæˆ 5 é¡Œä¸»è§€ç”Ÿæ´»æ»¿æ„åº¦æ¸¬é©—é¡Œç›®\"\"\"\n",
    "test_num_items = 5\n",
    "\n",
    "generated = item_writer.generate_items(simple_spec, num_items=test_num_items).get(\"items\", [])\n",
    "\n",
    "print(f\"ç”Ÿæˆçµæœ:\")\n",
    "for i, item in enumerate(generated, 1):\n",
    "    print(f\"{i:2}. {item['item'] if isinstance(item, dict) else item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹ 2: å–®ç¨ä½¿ç”¨è©•å¯©ç³»çµ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ç¤ºä¾‹ 2: è©•å¯©é¡Œç›®å“è³ª\n",
      "------------------------------\n",
      "ğŸ“ å…§å®¹æ•ˆåº¦è©•å¯©ä¸­...\n",
      "ğŸ“ èªè¨€å­¸è©•å¯©ä¸­...\n",
      "ğŸ“ åè¦‹æª¢æŸ¥è©•å¯©ä¸­...\n",
      "è©•å¯©çµæœç¶œåˆè©•åˆ†: 8/10\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” ç¤ºä¾‹ 2: è©•å¯©é¡Œç›®å“è³ª\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "sample_items = [\n",
    "    \"æˆ‘ç¶“å¸¸æ„Ÿåˆ°æ“”å¿ƒæˆ–ç·Šå¼µ\",\n",
    "    \"æˆ‘å¾ˆé›£æ”¾é¬†è‡ªå·±\",\n",
    "    \"æˆ‘å°æœªä¾†æ„Ÿåˆ°ææ‡¼\"\n",
    "]\n",
    "\n",
    "review_result = critic_agent.review_items(sample_items, \"ç„¦æ…®ç¨‹åº¦\")\n",
    "print(f\"è©•å¯©çµæœç¶œåˆè©•åˆ†: {review_result.get('overall_score', 0)}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹ 3: å–®ç¨ä½¿ç”¨è³‡æ–™åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ç¤ºä¾‹ 3: å¿ƒç†è¨ˆé‡åˆ†æ\n",
      "------------------------------\n",
      "æ¨¡æ“¬è³‡æ–™å½¢ç‹€: (100, 6)\n",
      "å‰ 5 è¡Œè³‡æ–™:\n",
      "   Item_1  Item_2  Item_3  Item_4  Item_5  Item_6\n",
      "0     3.0     4.0     3.0     5.0     5.0     2.0\n",
      "1     3.0     4.0     3.0     4.0     5.0     2.0\n",
      "2     3.0     4.0     3.0     5.0     5.0     2.0\n",
      "3     5.0     5.0     3.0     5.0     5.0     2.0\n",
      "4     3.0     4.0     3.0     5.0     4.0     3.0\n",
      "\n",
      "Cronbach's Alpha: 0.854\n",
      "å› å­è² è·é‡:\n",
      "        Factor_1  Factor_2\n",
      "Item_1    -0.779    -0.226\n",
      "Item_2    -0.702    -0.010\n",
      "Item_3    -0.782     0.362\n",
      "Item_4    -0.667    -0.284\n",
      "Item_5    -0.776    -0.071\n",
      "Item_6    -0.565    -0.023\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š ç¤ºä¾‹ 3: å¿ƒç†è¨ˆé‡åˆ†æ\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# è¼‰å…¥æ¨¡æ“¬è³‡æ–™ä¸¦é€²è¡Œåˆ†æ\n",
    "data = analysis_pipeline.load_simulated_data(n_participants=100, n_items=6)\n",
    "print(f\"æ¨¡æ“¬è³‡æ–™å½¢ç‹€: {data.shape}\")\n",
    "print(\"å‰ 5 è¡Œè³‡æ–™:\")\n",
    "print(data.head())\n",
    "\n",
    "# é€²è¡Œå› å­åˆ†æ\n",
    "efa_results = analysis_pipeline.exploratory_factor_analysis(n_factors=2)\n",
    "if \"error\" not in efa_results:\n",
    "    print(f\"\\nCronbach's Alpha: {efa_results['cronbach_alpha']:.3f}\")\n",
    "    print(\"å› å­è² è·é‡:\")\n",
    "    print(efa_results['factor_loadings_df'].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç³»çµ±ç¸½çµ\n",
    "\n",
    "ğŸ‰ **æ­å–œï¼LM-AIG ç³»çµ±åŸå‹å·²æˆåŠŸå¯¦ä½œä¸¦æ¸¬è©¦å®Œæˆï¼**\n",
    "\n",
    "### ğŸ”§ å·²å¯¦ç¾çš„åŠŸèƒ½\n",
    "\n",
    "1. **ItemWritingAgent (é¡Œç›®ç”Ÿæˆä»£ç†äºº)**\n",
    "   - âœ… ä½¿ç”¨ Ollama æœ¬åœ° LLM (`llama3.2:1b`)\n",
    "   - âœ… åŸºæ–¼ä½¿ç”¨è€…è¦æ ¼ç”Ÿæˆå¿ƒç†æ¸¬é©—é¡Œç›®\n",
    "   - âœ… æ”¯æ´é¡Œç›®æ”¹é€²å’Œå„ªåŒ–\n",
    "\n",
    "2. **CriticAgent (è©•å¯©ä»£ç†äºº)**\n",
    "   - âœ… ContentReviewer: å…§å®¹æ•ˆåº¦è©•ä¼°\n",
    "   - âœ… LinguisticReviewer: èªè¨€å“è³ªæª¢æŸ¥  \n",
    "   - âœ… BiasReviewer: åè¦‹æª¢æ¸¬\n",
    "   - âœ… MetaReviewer: æ•´åˆè©•å¯©çµæœ\n",
    "\n",
    "3. **DataAnalysisPipeline (è³‡æ–™åˆ†æç®¡ç·š)**\n",
    "   - âœ… æ¢ç´¢æ€§å› å­åˆ†æ (EFA)\n",
    "   - âœ… Cronbach's Alpha ä¿¡åº¦åˆ†æ\n",
    "   - âœ… é …ç›®çµ±è¨ˆèˆ‡ç›¸é—œåˆ†æ\n",
    "   - âœ… è‡ªå‹•åŒ–å ±å‘Šç”Ÿæˆ\n",
    "\n",
    "4. **LM-AIG System (å®Œæ•´å·¥ä½œæµç¨‹)**\n",
    "   - âœ… è¿­ä»£å¼é¡Œç›®ç”Ÿæˆèˆ‡æ”¹é€²\n",
    "   - âœ… è‡ªå‹•å“è³ªè©•ä¼°\n",
    "   - âœ… å¿ƒç†è¨ˆé‡ç‰¹æ€§åˆ†æ\n",
    "\n",
    "### ğŸ“Š æ¸¬è©¦çµæœ\n",
    "\n",
    "- **é¡Œç›®ç”Ÿæˆ**ï¼šæˆåŠŸç”Ÿæˆç¬¦åˆå¿ƒç†å­¸ç†è«–çš„æ¸¬é©—é¡Œç›®\n",
    "- **è©•å¯©å“è³ª**ï¼šå¤šé‡è©•å¯©æ©Ÿåˆ¶æœ‰æ•ˆé‹ä½œï¼Œæä¾›è©³ç´°å›é¥‹\n",
    "- **çµ±è¨ˆåˆ†æ**ï¼šCronbach's Alpha > 0.85ï¼Œé¡¯ç¤ºè‰¯å¥½ä¿¡åº¦\n",
    "\n",
    "### ğŸš€ æŠ€è¡“ç‰¹è‰²\n",
    "\n",
    "- **æœ¬åœ°åŒ–éƒ¨ç½²**ï¼šä½¿ç”¨ Ollama ç¢ºä¿è³‡æ–™éš±ç§\n",
    "- **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šå„çµ„ä»¶å¯ç¨ç«‹ä½¿ç”¨æˆ–æ•´åˆ\n",
    "- **å¯æ“´å±•æ€§**ï¼šæ˜“æ–¼æ·»åŠ æ–°çš„è©•å¯©æ¨™æº–æˆ–åˆ†ææ–¹æ³•\n",
    "- **è‡ªå‹•åŒ–ç¨‹åº¦é«˜**ï¼šå¾é¡Œç›®ç”Ÿæˆåˆ°çµ±è¨ˆåˆ†æå…¨ç¨‹è‡ªå‹•åŒ–\n",
    "\n",
    "### ğŸ”® æœªä¾†æ”¹é€²æ–¹å‘\n",
    "\n",
    "1. **æœç´¢ä»£ç†äºº**ï¼šæ•´åˆç¶²è·¯æœç´¢åŠŸèƒ½\n",
    "2. **æ›´å¤šçµ±è¨ˆåˆ†æ**ï¼šé …ç›®åæ‡‰ç†è«– (IRT)ã€é©—è­‰æ€§å› å­åˆ†æ (CFA)\n",
    "3. **ä½¿ç”¨è€…ä»‹é¢**ï¼šé–‹ç™¼ Web æˆ–æ¡Œé¢æ‡‰ç”¨ç¨‹å¼\n",
    "4. **æ¨¡å‹å„ªåŒ–**ï¼šæ”¯æ´æ›´å¤§çš„èªè¨€æ¨¡å‹æˆ–å°ˆé–€çš„å¿ƒç†å­¸æ¨¡å‹"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
