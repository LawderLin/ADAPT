# å°å…¥å¿…è¦çš„å¥—ä»¶
import ollama
from dataclasses import dataclass

# è¨­å®šåŸºæœ¬é…ç½®
@dataclass
class Config:
    """ç³»çµ±é…ç½®é¡åˆ¥"""
    model_name: str = "qwen3:8b"  # ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹
    temperature: float = 0.7
    max_tokens: int = 1000
    language: str = "ç¹é«”ä¸­æ–‡"

config = Config()

print("ğŸš€ AI è‡ªå‹•å‡ºé¡Œç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
print(f"ä½¿ç”¨æ¨¡å‹: {config.model_name}")

# æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦é‹è¡Œ
try:
    models_response = ollama.list()
    print(f"âœ… Ollama æœå‹™æ­£å¸¸")

    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    available_models = []
    if 'models' in models_response:
        available_models = [model.get('name', '')
                            for model in models_response['models']]

    print(f"å·²å®‰è£æ¨¡å‹: {available_models}")

    # æ¸¬è©¦æ¨¡å‹èª¿ç”¨
    test_response = ollama.chat(
        model=config.model_name,
        messages=[
            {"role": "user", "content": "Hello, please respond with 'System ready'"}],
        options={"temperature": 0.1, "num_predict": 10}
    )

    if test_response and 'message' in test_response:
        print(f"âœ… æ¨¡å‹ {config.model_name} æ¸¬è©¦æˆåŠŸï¼")
        print(f"æ¸¬è©¦å›æ‡‰: {test_response['message']['content']}")
    else:
        print("âš ï¸  æ¨¡å‹å›æ‡‰æ ¼å¼ç•°å¸¸")

except Exception as e:
    print(f"âŒ Ollama é€£æ¥å¤±æ•—: {e}")
    print("è«‹ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸¦ä¸”æ¨¡å‹å·²å®‰è£")
    print("å¯ä»¥å˜—è©¦é‹è¡Œ: ollama pull llama3.2:1b")