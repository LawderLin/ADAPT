"""
CFA Agent - é©—è­‰æ€§å› ç´ åˆ†æèˆ‡å› ç´ å‘½åä»£ç†äºº

åŠŸèƒ½ï¼š
1. åŸ·è¡Œé©—è­‰æ€§å› ç´ åˆ†æ (CFA)
2. åŸºæ–¼ç†è«–èƒŒæ™¯é€²è¡Œå› ç´ å‘½å
3. ç”Ÿæˆè©³ç´°çš„åˆ†æå ±å‘Š
4. æä¾›é¡Œç›®åˆªæ”¹å»ºè­°
5. æ¨è–¦æœ€ä½³å› ç´ æ•¸é‡èˆ‡åç¨±
"""

from typing import Dict, List, Any, Optional
import numpy as np
import pandas as pd
from sklearn.decomposition import FactorAnalysis
from sklearn.preprocessing import StandardScaler

from config import config

class CFAAgent:
    """
    åŸ·è¡Œå› ç´ åˆ†æã€å› ç´ å‘½åã€ä»¥åŠæä¾›æ¸¬é©—æ”¹é€²å»ºè­°
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ– CFA ä»£ç†äºº
        """
        self.model = config.model_name
        self.system_prompt = """You are an expert in psychometrics and factor analysis. Your task is to assist in performing Confirmatory Factor Analysis (CFA) on psychological test data, naming the factors based on theoretical background, and providing suggestions for test item improvement."""
        self.cfa_results = None
        self.data = None
        self.scaler = StandardScaler()
        
    def analyze(self, 
                theoretical_background: str,
                test_items: list[str],
                data: pd.DataFrame,
                n_factors: Optional[int] = None,
                name_with_llm: bool = True) -> Dict[str, Any]:
        """
        åŸ·è¡Œå®Œæ•´çš„é©—è­‰æ€§å› ç´ åˆ†ææµç¨‹
        
        Args:
            theoretical_background: æ¸¬é©—çš„ç†è«–èƒŒæ™¯
            data: æ¸¬é©—åŸå§‹æ•¸æ“š (DataFrame)
            n_factors: å› ç´ æ•¸é‡ (è‹¥ç‚º None å‰‡è‡ªå‹•åˆ¤æ–·)
            
        Returns:
            åŒ…å«åˆ†æçµæœã€å»ºè­°å’Œå› ç´ å‘½åçš„å®Œæ•´å ±å‘Š
        """
        print("ğŸ” é–‹å§‹é©—è­‰æ€§å› ç´ åˆ†ææµç¨‹...")
        
        self.data = data
        
        # 1. è³‡æ–™é è™•ç†
        print("ğŸ“Š è³‡æ–™é è™•ç†ä¸­...")
        processed_data = self._preprocess_data(data)
        
        # 2. ç¢ºå®šå› ç´ æ•¸é‡
        if n_factors is None:
            print("ğŸ”¢ è‡ªå‹•åˆ¤æ–·å› ç´ æ•¸é‡...")
            n_factors = self._determine_optimal_factors(processed_data)
            print(f"âœ… æ¨è–¦å› ç´ æ•¸é‡: {n_factors}")
        
        # 3. åŸ·è¡Œå› ç´ åˆ†æ
        print("ğŸ“ˆ åŸ·è¡Œå› ç´ åˆ†æ...")
        cfa_results = self._perform_cfa(processed_data, n_factors)
        
        # 4. å› ç´ å‘½å (ä½¿ç”¨ LLM)
        print("ğŸ·ï¸  é€²è¡Œå› ç´ å‘½å...")
        factor_names = self._name_factors(
            theoretical_background=theoretical_background,
            test_items=test_items,
            loadings=cfa_results['loadings_df'],
            n_factors=n_factors,
            name_with_llm=name_with_llm
        )
        
        # 5. è©•ä¼°å› ç´ å“è³ª
        print("â­ è©•ä¼°å› ç´ å“è³ª...")
        quality_assessment = self._assess_factor_quality(cfa_results)
        
        # 6. ç”Ÿæˆé¡Œç›®æ”¹é€²å»ºè­°
        print("âœï¸  ç”Ÿæˆé¡Œç›®æ”¹é€²å»ºè­°...")
        item_suggestions = self._generate_item_suggestions(
            cfa_results,
            factor_names
        )
        
        # 7. ç”Ÿæˆå®Œæ•´å ±å‘Š
        print("ğŸ“‹ ç”Ÿæˆåˆ†æå ±å‘Š...")
        report = self._generate_comprehensive_report(
            theoretical_background=theoretical_background,
            cfa_results=cfa_results,
            factor_names=factor_names,
            quality_assessment=quality_assessment,
            item_suggestions=item_suggestions,
            n_factors=n_factors
        )
        
        # çµ„åˆæœ€çµ‚çµæœ
        final_results = {
            "status": "success",
            "theoretical_background": theoretical_background,
            "n_factors": n_factors,
            "factor_names": factor_names,
            "cfa_results": {
                "cronbach_alpha": cfa_results['cronbach_alpha'],
                "loadings": cfa_results['loadings_df'].round(3).to_dict(),
                "variance_explained": cfa_results['explained_variance'],
                "correlation_matrix": cfa_results['correlation_matrix'].round(3).to_dict()
            },
            "quality_assessment": quality_assessment,
            "item_suggestions": item_suggestions,
            "report": report
        }
        
        print("âœ… åˆ†æå®Œæˆï¼")
        
        return final_results
    
    def _preprocess_data(self, data: pd.DataFrame) -> np.ndarray:
        """
        è³‡æ–™é è™•ç†ï¼šæ¨™æº–åŒ–å’Œç¼ºå€¼è™•ç†
        """
        # è™•ç†ç¼ºå€¼
        data_clean = data.dropna()
        
        if len(data_clean) < len(data):
            print(f"âš ï¸  ç§»é™¤äº† {len(data) - len(data_clean)} åˆ—å«ç¼ºå€¼çš„è³‡æ–™")
        
        # æ¨™æº–åŒ–
        scaled_data = self.scaler.fit_transform(data_clean)
        
        return scaled_data
    
    def _determine_optimal_factors(self, data: np.ndarray) -> int:
        """
        åŸºæ–¼ç¢çŸ³åœ– (Scree Plot) å’Œ Kaiser æº–å‰‡åˆ¤æ–·æœ€ä½³å› ç´ æ•¸é‡
        """
        # è¨ˆç®—ç‰¹å¾µå€¼
        correlation_matrix = np.corrcoef(data.T)
        eigenvalues = np.linalg.eigvalsh(correlation_matrix)
        eigenvalues = np.sort(eigenvalues)[::-1]
        
        # Kaiser æº–å‰‡ï¼šç‰¹å¾µå€¼ > 1
        kaiser_criterion = np.sum(eigenvalues > 1)
        
        # æ–¹å·®è§£é‡‹æº–å‰‡ï¼šè‡³å°‘è§£é‡‹ 70% çš„æ–¹å·®
        cumsum_variance = np.cumsum(eigenvalues) / np.sum(eigenvalues)
        variance_criterion = np.argmax(cumsum_variance >= 0.7) + 1
        
        # å–å…©å€‹æº–å‰‡çš„å¹³å‡å€¼
        optimal_factors = max(1, int(np.round((kaiser_criterion + variance_criterion) / 2)))
        
        return min(optimal_factors, data.shape[1] - 1)
    
    def _perform_cfa(self, data: np.ndarray, n_factors: int) -> Dict[str, Any]:
        """
        åŸ·è¡Œå› ç´ åˆ†æ
        """
        # å› ç´ åˆ†æ
        fa = FactorAnalysis(n_components=n_factors, random_state=42, max_iter=1000)
        factor_scores = fa.fit_transform(data)
        
        # å› ç´ è² è·é‡
        loadings = fa.components_.T
        
        # è¨ˆç®—è§£é‡‹æ–¹å·®
        explained_variance = []
        for i in range(n_factors):
            var = np.var(factor_scores[:, i]) if factor_scores.shape[1] > i else 0
            explained_variance.append(var)
        
        total_variance = np.sum(explained_variance)
        variance_ratio = [v / total_variance for v in explained_variance] if total_variance > 0 else [0] * n_factors
        
        # Cronbach's Alpha
        alpha = self._calculate_cronbach_alpha(data)
        
        # ç›¸é—œçŸ©é™£
        correlation_matrix = np.corrcoef(data.T)
        
        # å»ºç«‹è² è·é‡ DataFrame
        loadings_df = pd.DataFrame(
            loadings,
            columns=[f'Factor_{i+1}' for i in range(n_factors)],
            index=[f'Item_{i+1}' for i in range(data.shape[1])]
        )
        
        return {
            "loadings": loadings,
            "loadings_df": loadings_df,
            "factor_scores": factor_scores,
            "explained_variance": variance_ratio,
            "correlation_matrix": pd.DataFrame(correlation_matrix),
            "cronbach_alpha": alpha,
            "n_factors": n_factors
        }
    
    def _calculate_cronbach_alpha(self, data: np.ndarray) -> float:
        """
        è¨ˆç®— Cronbach's Alpha ä¿¡åº¦ä¿‚æ•¸
        """
        k = data.shape[1]
        if k < 2:
            return 0.0
        
        item_variances = np.var(data, axis=0, ddof=1)
        total_variance = np.var(np.sum(data, axis=1), ddof=1)
        
        if total_variance == 0:
            return 0.0
        
        alpha = (k / (k - 1)) * (1 - np.sum(item_variances) / total_variance)
        
        return max(0, min(1, alpha))  # é™åˆ¶åœ¨ 0-1 ä¹‹é–“
    
    def _name_factors(self,
                     theoretical_background: str,
                     test_items: list[str],
                     loadings: pd.DataFrame,
                     n_factors: int,
                     name_with_llm: bool = True) -> Dict[str, str]:
        """
        ä½¿ç”¨ LLM æ ¹æ“šç†è«–èƒŒæ™¯å’Œå› ç´ è² è·é‡å‘½åå› ç´ 
        """
        if not name_with_llm:
            return {f"Factor_{i+1}": f"Factor_{i+1}" for i in range(n_factors)}
        
        try:
            import ollama
        except ImportError:
            print("âš ï¸  ç„¡ Ollama å®¢æˆ¶ç«¯ï¼Œä½¿ç”¨é è¨­å› ç´ å‘½å")
            return {f"Factor_{i+1}": f"Factor_{i+1}" for i in range(n_factors)}
        
        # æå–å„å› ç´ æœ€é«˜è² è·é‡çš„é …ç›®
        factor_descriptions = []
        for idx, factor_col in enumerate(loadings.columns, 1):
            # æ‰¾å‡ºè² è·é‡æœ€é«˜çš„ 3-5 å€‹é¡Œç›®
            top_k = min(5, len(loadings))
            top_indices = loadings[factor_col].abs().nlargest(top_k).index
            top_descriptions = []
            for item_idx in top_indices:
                loading = loadings.loc[item_idx, factor_col]
                item_text = test_items[int(item_idx.split('_')[1]) - 1] if len(test_items) > 0 else f"{item_idx}"
                top_descriptions.append(f"    - {item_text[:80]} (è² è·é‡: {loading:.3f})")
            
            description = f"\nFactor_{idx} çš„ä¸»è¦é¡Œç›®ï¼š\n" + "\n".join(top_descriptions)
            factor_descriptions.append(description)
        
        prompt = f"""ä½ æ˜¯å¿ƒç†æ¸¬é©—å°ˆå®¶ã€‚æ ¹æ“šä»¥ä¸‹ç†è«–èƒŒæ™¯å’Œå› ç´ åˆ†æçµæœï¼Œè«‹ç‚ºæ¯å€‹å› ç´ å‘½åã€‚

ç†è«–èƒŒæ™¯ï¼š
{theoretical_background}

å› ç´ åˆ†æçµæœï¼š
{''.join(factor_descriptions)}

è«‹ä¾åºç‚ºæ¯å€‹å› ç´ æä¾›ä¸€å€‹ç°¡æ½”çš„ä¸­æ–‡åç¨±ï¼ˆ2-6 å­—ï¼‰ï¼Œç›´æ¥è¼¸å‡ºåç¨±åˆ—è¡¨ã€‚
æ ¼å¼ä¾‹å¦‚ï¼š
å¤šæ–¹æ€è€ƒ
æ•´åˆæ€§
å’Œè«§æ€§

åªè¼¸å‡ºå› ç´ åç¨±ï¼Œæ¯å€‹åç¨±å ä¸€è¡Œã€‚
"""
        
        factor_names = {}
        try:
            response = ollama.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­çš„å¿ƒç†æ¸¬é©—åˆ†æå¸«ï¼Œæ“…é•·æ ¹æ“šé¡Œé …å…§å®¹å’Œç†è«–èƒŒæ™¯ç‚ºæ½›åœ¨å› ç´ å‘½åã€‚"},
                    {"role": "user", "content": prompt}
                ],
                options={"temperature": 0.3, "num_predict": 500},
                think=False
            )
            
            content = response['message']['content'].strip()
            
            # ç°¡å–®è§£æï¼šæ¯è¡Œä¸€å€‹å› ç´ åç¨±
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            names = [line for line in lines if line and len(line) <= 20 and not line.startswith('#')]
            
            # æ˜ å°„å› ç´ åç¨±
            for i in range(min(n_factors, len(names))):
                factor_names[f"Factor_{i+1}"] = names[i]
        
        except Exception as e:
            print(f"âš ï¸  LLM å‘½åå¤±æ•—: {e}ï¼Œä½¿ç”¨é è¨­åç¨±")
        
        # è£œå……ç¼ºå¤±çš„å› ç´ 
        for i in range(n_factors):
            if f"Factor_{i+1}" not in factor_names:
                factor_names[f"Factor_{i+1}"] = f"Factor_{i+1}"
        
        return factor_names
    
    def _assess_factor_quality(self, cfa_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        è©•ä¼°å› ç´ å“è³ªï¼ˆå› ç´ è² è·é‡ã€ä¿¡åº¦ç­‰ï¼‰
        """
        loadings_df = cfa_results['loadings_df']
        n_factors = cfa_results['n_factors']
        
        assessment = {
            "overall_quality": "Good",
            "reliability": {
                "cronbach_alpha": cfa_results['cronbach_alpha'],
                "interpretation": self._interpret_alpha(cfa_results['cronbach_alpha'])
            },
            "loading_quality": {}
        }
        
        # æª¢æŸ¥æ¯å€‹å› ç´ çš„è² è·é‡å“è³ª
        for factor in loadings_df.columns:
            factor_loadings = loadings_df[factor].abs()
            high_loadings = (factor_loadings >= 0.5).sum()
            mean_loading = factor_loadings.mean()
            min_loading = factor_loadings.min()
            
            assessment["loading_quality"][factor] = {
                "high_loading_items": int(high_loadings),
                "mean_loading": float(mean_loading),
                "min_loading": float(min_loading),
                "quality": "Excellent" if mean_loading >= 0.6 else "Good" if mean_loading >= 0.5 else "Fair"
            }
        
        # æ•´é«”è©•åƒ¹
        mean_alpha = cfa_results['cronbach_alpha']
        if mean_alpha >= 0.8:
            assessment["overall_quality"] = "Excellent"
        elif mean_alpha >= 0.7:
            assessment["overall_quality"] = "Good"
        elif mean_alpha >= 0.6:
            assessment["overall_quality"] = "Fair"
        else:
            assessment["overall_quality"] = "Poor"
        
        return assessment
    
    def _interpret_alpha(self, alpha: float) -> str:
        """
        è§£é‡‹ Cronbach's Alpha å€¼
        """
        if alpha >= 0.9:
            return "å„ªç•° (Î± â‰¥ 0.9)"
        elif alpha >= 0.8:
            return "è‰¯å¥½ (0.8 â‰¤ Î± < 0.9)"
        elif alpha >= 0.7:
            return "å¯æ¥å— (0.7 â‰¤ Î± < 0.8)"
        elif alpha >= 0.6:
            return "å‹‰å¼·å¯æ¥å— (0.6 â‰¤ Î± < 0.7)"
        else:
            return "ä¸å¯æ¥å— (Î± < 0.6)ï¼Œéœ€è¦æ”¹é€²"
    
    def _generate_item_suggestions(self,
                                   cfa_results: Dict[str, Any],
                                   factor_names: Dict[str, str]) -> Dict[str, List[str]]:
        """
        æ ¹æ“šå› ç´ è² è·é‡ç”Ÿæˆé¡Œç›®æ”¹é€²å»ºè­°
        """
        loadings_df = cfa_results['loadings_df']
        suggestions = {
            "items_to_remove": [],
            "items_to_review": [],
            "items_to_keep": []
        }
        
        for item in loadings_df.index:
            max_loading = loadings_df.loc[item].abs().max()
            
            if max_loading < 0.3:
                suggestions["items_to_remove"].append(f"{item} (æœ€é«˜è² è·é‡: {max_loading:.3f})")
            elif max_loading < 0.5:
                suggestions["items_to_review"].append(f"{item} (æœ€é«˜è² è·é‡: {max_loading:.3f})")
            else:
                suggestions["items_to_keep"].append(f"{item} (æœ€é«˜è² è·é‡: {max_loading:.3f})")
        
        return suggestions
    
    def _generate_comprehensive_report(self,
                                       theoretical_background: str,
                                       cfa_results: Dict[str, Any],
                                       factor_names: Dict[str, str],
                                       quality_assessment: Dict[str, Any],
                                       item_suggestions: Dict[str, List[str]],
                                       n_factors: int) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„åˆ†æå ±å‘Š
        """
        report = f"""
{'='*70}
ğŸ“Š é©—è­‰æ€§å› ç´ åˆ†æ (CFA) è©³ç´°å ±å‘Š
{'='*70}

ğŸ¯ ç†è«–èƒŒæ™¯
{'-'*70}
{theoretical_background}

ğŸ“ˆ å› ç´ çµæ§‹
{'-'*70}
å› ç´ æ•¸é‡: {n_factors}
å› ç´ åç¨±: {', '.join(factor_names.values())}

ğŸ“‹ ä¿¡åº¦åˆ†æ
{'-'*70}
Cronbach's Alpha: {quality_assessment['reliability']['cronbach_alpha']:.4f}
è§£é‡‹: {quality_assessment['reliability']['interpretation']}
æ•´é«”å“è³ªè©•ç´š: {quality_assessment['overall_quality']}

ğŸ”¢ å› ç´ è² è·é‡åˆ†æ
{'-'*70}
{cfa_results['loadings_df'].round(3).to_string()}

ğŸ“Š è§£é‡‹æ–¹å·®æ¯”ä¾‹
{'-'*70}
{', '.join([f'Factor_{i+1}: {var:.1%}' for i, var in enumerate(cfa_results['explained_variance'])])}

â­ å› ç´ è² è·é‡å“è³ªè©•ä¼°
{'-'*70}
"""
        
        for factor, quality in quality_assessment['loading_quality'].items():
            report += f"\n{factor}:"
            report += f"\n  - é«˜è² è·é‡é¡Œç›®æ•¸: {quality['high_loading_items']}"
            report += f"\n  - å¹³å‡è² è·é‡: {quality['mean_loading']:.3f}"
            report += f"\n  - å“è³ªè©•ç´š: {quality['quality']}"
        
        report += f"\n\nâœï¸  é¡Œç›®æ”¹é€²å»ºè­°\n{'-'*70}\n"
        
        if cfa_results['loadings_df'].shape[1] < cfa_results['loadings_df'].shape[0]:
            report += f"\nğŸ—‘ï¸  å»ºè­°åˆªé™¤çš„é¡Œç›® ({len(item_suggestions['items_to_remove'])} å€‹):\n"
            for item in item_suggestions['items_to_remove'][:10]:  # æœ€å¤šé¡¯ç¤º 10 å€‹
                report += f"  â€¢ {item}\n"
            if len(item_suggestions['items_to_remove']) > 10:
                report += f"  ... å…± {len(item_suggestions['items_to_remove'])} å€‹\n"
            
            report += f"\nâš ï¸  å»ºè­°å¯©è¦–çš„é¡Œç›® ({len(item_suggestions['items_to_review'])} å€‹):\n"
            for item in item_suggestions['items_to_review'][:10]:
                report += f"  â€¢ {item}\n"
            if len(item_suggestions['items_to_review']) > 10:
                report += f"  ... å…± {len(item_suggestions['items_to_review'])} å€‹\n"
            
            report += f"\nâœ… ä¿ç•™çš„é¡Œç›® ({len(item_suggestions['items_to_keep'])} å€‹):\n"
            for item in item_suggestions['items_to_keep'][:10]:
                report += f"  â€¢ {item}\n"
            if len(item_suggestions['items_to_keep']) > 10:
                report += f"  ... å…± {len(item_suggestions['items_to_keep'])} å€‹\n"
        
        report += f"\n\nğŸ’¡ å»ºè­°\n{'-'*70}\n"
        
        alpha = quality_assessment['reliability']['cronbach_alpha']
        if alpha >= 0.8:
            report += "âœ… æ¸¬é©—ä¿¡åº¦è‰¯å¥½ï¼Œå› ç´ çµæ§‹ç©©å®šï¼Œå¯ä»¥è€ƒæ…®é€²ä¸€æ­¥é©—è­‰\n"
        elif alpha >= 0.7:
            report += "âš ï¸  æ¸¬é©—ä¿¡åº¦å°šå¯ï¼Œå»ºè­°åˆªé™¤æˆ–ä¿®æ”¹ä½è² è·é‡çš„é¡Œç›®\n"
        else:
            report += "âŒ æ¸¬é©—ä¿¡åº¦åä½ï¼Œå¼·çƒˆå»ºè­°é€²è¡Œé¡Œç›®ä¿®è¨‚æˆ–å› ç´ çµæ§‹èª¿æ•´\n"
        
        report += f"\n{'='*70}\n"
        
        return report
